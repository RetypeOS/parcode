===== benches\lazy_bench.rs =====
//! benches/lazy_bench.rs
#![allow(missing_docs)]

use criterion::{Criterion, criterion_group, criterion_main};
use parcode::{Parcode, ParcodeObject, ParcodeReader};
use serde::{Deserialize, Serialize};
use std::hint::black_box;
use tempfile::NamedTempFile;

#[derive(Serialize, Deserialize, Clone, ParcodeObject)]
struct HeavyNode {
    /// Metadata field
    meta: u64,
    /// Heavy payload
    #[parcode(chunkable)]
    payload: Vec<u8>,
}

#[derive(Serialize, Deserialize, Clone, ParcodeObject)]
struct Root {
    /// First child
    #[parcode(chunkable)]
    child_a: HeavyNode,
    /// Second child
    #[parcode(chunkable)]
    child_b: HeavyNode,
}

/// PLACEHOLDER
fn bench_lazy(c: &mut Criterion) {
    let data = Root {
        child_a: HeavyNode {
            meta: 1,
            payload: vec![0; 1_000_000],
        },
        child_b: HeavyNode {
            meta: 2,
            payload: vec![0; 1_000_000],
        },
    };

    let file = NamedTempFile::new().unwrap();
    Parcode::save(file.path(), &data).unwrap();
    let path = file.path().to_owned();

    let mut group = c.benchmark_group("Lazy Access");

    // Caso A: Carga Completa (Estándar)
    group.bench_function("full_load", |b| {
        b.iter(|| {
            let loaded: Root = Parcode::read(&path).unwrap();
            black_box(loaded.child_a.meta);
        })
    });

    // Caso B: Carga Lazy (Solo metadatos)
    group.bench_function("lazy_meta_only", |b| {
        b.iter(|| {
            let reader = ParcodeReader::open(&path).unwrap();
            let lazy = reader.read_lazy::<Root>().unwrap();
            // Accedemos a meta profundo A y B
            let sum = lazy.child_a.meta + lazy.child_b.meta;
            black_box(sum);
        })
    });

    // Caso C: Carga Parcial (Meta A + Payload A)
    group.bench_function("lazy_partial_load", |b| {
        b.iter(|| {
            let reader = ParcodeReader::open(&path).unwrap();
            let lazy = reader.read_lazy::<Root>().unwrap();
            let payload = lazy.child_a.payload.load().unwrap();
            black_box(payload.len());
        })
    });

    group.finish();
}

criterion_group!(benches, bench_lazy);
criterion_main!(benches);

===== benches\map_access.rs =====
// benches/map_access.rs
//! PLACEHOLDER
//!
#![allow(missing_docs)]
use criterion::{Criterion, criterion_group, criterion_main};
use parcode::{Parcode, ParcodeObject, ParcodeReader};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tempfile::NamedTempFile;

#[derive(Serialize, Deserialize, ParcodeObject)]
struct MapContainer {
    /// PLACEHOLDER
    #[parcode(map)] // Optimized
    opt_map: HashMap<u64, u64>,
    // #[parcode(chunkable)] // Standard Blob (Unoptimized for random access)
    // std_map: HashMap<u64, u64>,
    // Nota: Para comparar justo, deberíamos usar otro campo o struct,
    // ya que HashMap sin 'map' flag se guarda como blob.
}

fn bench_map(c: &mut Criterion) {
    let count = 100_000;
    let mut map = HashMap::new();
    for i in 0..count {
        map.insert(i, i);
    }

    let data = MapContainer { opt_map: map };
    let file = NamedTempFile::new().unwrap();
    Parcode::save(file.path(), &data).unwrap();
    let path = file.path().to_owned();

    let mut group = c.benchmark_group("Map Random Access");

    group.bench_function("optimized_lookup", |b| {
        // Setup reader once per batch to simulate persistent app
        let reader = ParcodeReader::open(&path).unwrap();
        let lazy = reader.read_lazy::<MapContainer>().unwrap();

        b.iter(|| {
            // Lookup key 50,000 (Middle)
            let val = lazy.opt_map.get(&50_000).unwrap();
            std::hint::black_box(val);
        })
    });

    group.finish();
}

criterion_group!(benches, bench_map);
criterion_main!(benches);

===== benches\performance.rs =====
// ===== benches\performance.rs =====
#![allow(missing_docs)]

use criterion::{Criterion, Throughput, criterion_group, criterion_main};
use parcode::{Parcode, ParcodeError, ParcodeReader, graph::*, visitor::ParcodeVisitor};
use serde::{Deserialize, Serialize};
use std::fs::File;
use std::hint::black_box;
use std::io::BufWriter;
use tempfile::NamedTempFile;

// --- SETUP ---

#[derive(Clone, Serialize, Deserialize)]
struct BenchItem {
    id: u64,
    payload: Vec<u64>, // 1KB payload
}

// Wrapper to satisfy Parcode traits
#[derive(Clone, Serialize, Deserialize)]
struct BenchCollection(Vec<BenchItem>);

// Minimal Manual Implementation for Benchmarking
impl ParcodeVisitor for BenchCollection {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        // Delegamos al Vec interno, propagando la configuración
        self.0.visit(graph, parent_id, config_override);
    }

    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        let base = Box::new(ContainerJob);
        if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(base, cfg))
        } else {
            base
        }
    }
}

impl ParcodeVisitor for BenchItem {
    fn visit(
        &self,
        _graph: &mut TaskGraph<'_>,
        _parent_id: Option<ChunkId>,
        _config_override: Option<JobConfig>,
    ) {
        // BenchItem is a simple leaf (payload).
        // It does NOT create new child nodes.
        // Its data is serialized within the VecShardJob of the parent collection.
    }

    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        // Not used if visit does not create nodes, but correct impl provided
        let base = Box::new(ItemJob(self.clone()));
        if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(base, cfg))
        } else {
            base
        }
    }
}

#[derive(Clone)]
struct ContainerJob;
impl SerializationJob<'_> for ContainerJob {
    fn execute(&self, _: &[parcode::format::ChildRef]) -> parcode::Result<Vec<u8>> {
        Ok(vec![])
    }
    fn estimated_size(&self) -> usize {
        0
    }
}

#[derive(Clone)]
struct ItemJob(BenchItem);
impl SerializationJob<'_> for ItemJob {
    fn execute(&self, _: &[parcode::format::ChildRef]) -> parcode::Result<Vec<u8>> {
        bincode::serde::encode_to_vec(&self.0, bincode::config::standard())
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }
    fn estimated_size(&self) -> usize {
        1024
    }
}

fn generate_data(count: usize) -> BenchCollection {
    let items = (0..count)
        .map(|i| BenchItem {
            id: i as u64,
            payload: vec![i as u64; 128], // ~1KB
        })
        .collect();
    BenchCollection(items)
}

// --- BENCHMARKS ---

fn bench_writers(c: &mut Criterion) {
    let item_count = 200_000;
    let data = generate_data(item_count);
    let raw_data = &data.0; // For bincode

    println!("Writers Item count: {}", item_count);

    let mut group = c.benchmark_group("Serialization Write");
    group.throughput(Throughput::Bytes((item_count * 1032) as u64));

    // 1. Baseline: Bincode (Single Threaded)
    group.bench_function("bincode_serialize", |b| {
        b.iter(|| {
            let file = NamedTempFile::new().unwrap();
            let mut writer = BufWriter::new(file);
            bincode::serde::encode_into_std_write(
                black_box(raw_data),
                &mut writer,
                bincode::config::standard(),
            )
            .unwrap();
        })
    });

    // 2. Parcode (Parallel Graph Engine)
    group.bench_function("parcode_save", |b| {
        b.iter(|| {
            let file = NamedTempFile::new().unwrap();
            Parcode::save(file.path(), black_box(&data)).unwrap();
        })
    });

    group.finish();
}

fn bench_readers(c: &mut Criterion) {
    let item_count = 200_000;

    println!("Readers Item count: {}", item_count);

    let data = generate_data(item_count);

    // Setup files
    let bincode_file = NamedTempFile::new().unwrap();
    bincode::serde::encode_into_std_write(
        &data.0,
        &mut BufWriter::new(&bincode_file),
        bincode::config::standard(),
    )
    .unwrap();
    let bincode_path = bincode_file.path().to_owned();

    let parcode_file = NamedTempFile::new().unwrap();
    Parcode::save(parcode_file.path(), &data).unwrap();
    let parcode_path = parcode_file.path().to_owned();

    let reader = ParcodeReader::open(&parcode_path).unwrap();
    let root = reader.root().unwrap();
    println!("Chunks detected: {}", root.children().unwrap().len());

    let mut group = c.benchmark_group("Deserialization Read");

    // 1. Bincode: Standard
    group.bench_function("bincode_read_all", |b| {
        b.iter(|| {
            let file = File::open(&bincode_path).unwrap();
            let _res: Vec<BenchItem> = bincode::serde::decode_from_std_read(
                &mut std::io::BufReader::new(file),
                bincode::config::standard(),
            )
            .unwrap();
        })
    });

    // 2. Parcode: Random Access (Item único)
    group.bench_function("parcode_random_access_10", |b| {
        b.iter(|| {
            let reader = ParcodeReader::open(&parcode_path).unwrap();
            let root = reader.root().unwrap();

            // Usamos la API de alto nivel get_item
            for i in (0..10).map(|x| x * (item_count / 10)) {
                let _obj: BenchItem = root.get(i).unwrap();
            }
        })
    });

    // 3. Parcode: Full Scan (Manual Shard Iteration)
    group.bench_function("parcode_full_scan_manual", |b| {
        b.iter(|| {
            let reader = ParcodeReader::open(&parcode_path).unwrap();
            let root = reader.root().unwrap();

            // Obtenemos los Shards (Hijos directos)
            let shards = root.children().unwrap();

            for shard_node in shards {
                // Deserializamos el Shard completo (Vec<BenchItem>)
                let items: Vec<BenchItem> = shard_node.decode().unwrap();

                // Iteramos los items en memoria (simulando uso)
                for item in items {
                    black_box(item);
                }
            }
        })
    });

    // 4. Parcode: Parallel Stitching (La nueva joya)
    // Añadimos esto para probar la velocidad de reconstrucción total
    group.bench_function("parcode_read_all_parallel", |b| {
        b.iter(|| {
            let reader = ParcodeReader::open(&parcode_path).unwrap();
            let root = reader.root().unwrap();
            // Reconstruye el Vec<BenchItem> completo usando todos los cores
            let _res: Vec<BenchItem> = root.decode_parallel_collection().unwrap();
        })
    });

    group.finish();
}

criterion_group!(benches, bench_writers, bench_readers);
criterion_main!(benches);

===== examples\audit.rs =====
// examples/comprehensive_audit.rs
//! Comprehensive Architectural Audit: Parcode V3 vs Bincode.
//! Complex nested structures, mixed collections, no compression.
//! Run: cargo run --example audit --release

#![allow(missing_docs)]
#![allow(unsafe_code)]

use parcode::{Parcode, ParcodeObject, ParcodeReader};
use serde::{Deserialize, Serialize};
use std::alloc::{GlobalAlloc, Layout, System};
use std::collections::HashMap;
use std::fs::File;
use std::io::{BufReader, BufWriter};
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::Instant;
use tempfile::NamedTempFile;

// ============================================================================
// 1. MEMORY PROFILER
// ============================================================================

struct ProfilingAllocator<A: GlobalAlloc> {
    inner: A,
    allocated: AtomicUsize,
    peak: AtomicUsize,
}

impl<A: GlobalAlloc> ProfilingAllocator<A> {
    const fn new(inner: A) -> Self {
        Self {
            inner,
            allocated: AtomicUsize::new(0),
            peak: AtomicUsize::new(0),
        }
    }
    fn reset(&self) {
        self.allocated.store(0, Ordering::SeqCst);
        self.peak.store(0, Ordering::SeqCst);
    }
    fn peak(&self) -> usize {
        self.peak.load(Ordering::SeqCst)
    }
}

unsafe impl<A: GlobalAlloc> GlobalAlloc for ProfilingAllocator<A> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let ptr = unsafe { self.inner.alloc(layout) };
        if !ptr.is_null() {
            let current = self.allocated.fetch_add(layout.size(), Ordering::SeqCst) + layout.size();
            self.peak.fetch_max(current, Ordering::SeqCst);
        }
        ptr
    }
    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        self.allocated.fetch_sub(layout.size(), Ordering::SeqCst);
        unsafe { self.inner.dealloc(ptr, layout) };
    }
}

#[global_allocator]
static ALLOCATOR: ProfilingAllocator<System> = ProfilingAllocator::new(System);

// ============================================================================
// 2. COMPLEX DATA MODEL (Deep Nesting)
// ============================================================================

#[derive(Serialize, Deserialize, Clone, PartialEq, Debug, ParcodeObject)]
struct WorldState {
    id: u64,
    name: String,

    // Test: Acceso O(1) en Mapa
    #[parcode(chunkable)]
    users: HashMap<u64, UserProfile>,

    // Test: Lazy Navigation (Recursiva)
    #[parcode(chunkable)]
    regions: Vec<Region>,

    // Test: Streaming/Partial Load
    #[parcode(chunkable)]
    system_logs: Vec<String>,
}

#[derive(Serialize, Deserialize, Clone, PartialEq, Debug, ParcodeObject)]
struct Region {
    id: u32,
    name: String,
    // Nested Chunkable: Permite cargar una región sin cargar sus zonas
    #[parcode(chunkable)]
    zones: Vec<Zone>,
}

#[derive(Serialize, Deserialize, Clone, PartialEq, Debug, ParcodeObject)]
struct Zone {
    id: u32,
    // Payload pesado
    #[parcode(chunkable)]
    terrain_data: Vec<u8>,
}

#[derive(Serialize, Deserialize, Clone, PartialEq, Debug)]
struct UserProfile {
    username: String,
    xp: u64,
    bio: String,
}

// ============================================================================
// 3. GENERATOR
// ============================================================================

fn generate_world() -> WorldState {
    print!("Generating Complex World... ");
    let start = Instant::now();

    // 1. Users Map (Heavy Map)
    let mut users = HashMap::new();
    for i in 0..100_000 {
        users.insert(
            i,
            UserProfile {
                username: format!("Player_{}", i),
                xp: i * 100,
                bio: "A very long description tailored to fill some bytes in the bucket.".into(),
            },
        );
    }

    // 2. Deep Hierarchy (World -> Regions -> Zones -> Data)
    let regions = (0..10)
        .map(|r_id| {
            Region {
                id: r_id,
                name: format!("Region_{}", r_id),
                zones: (0..50)
                    .map(|z_id| {
                        Zone {
                            id: z_id,
                            terrain_data: vec![r_id as u8; 20_000], // 20KB per zone * 50 = 1MB per region
                        }
                    })
                    .collect(),
            }
        })
        .collect();

    // 3. Flat Logs
    let logs = (0..50_000)
        .map(|i| format!("Log entry #{} with some content", i))
        .collect();

    let w = WorldState {
        id: 1,
        name: "Azeroth_V3".into(),
        users,
        regions,
        system_logs: logs,
    };

    println!(
        "Done in {:.2?}. RAM Footprint: {:.2} MB",
        start.elapsed(),
        ALLOCATOR.peak() as f64 / 1024.0 / 1024.0
    );
    w
}

// ============================================================================
// 4. AUDIT EXECUTION
// ============================================================================

fn main() -> parcode::Result<()> {
    println!("\n=== PARCODE V3 COMPREHENSIVE ARCHITECTURAL AUDIT ===\n");

    ALLOCATOR.reset();
    let world_data = generate_world();

    let path_par = NamedTempFile::new().unwrap();
    let path_bin = NamedTempFile::new().unwrap();

    // ------------------------------------------------------------------------
    // TEST 1: WRITING (Serialization)
    // ------------------------------------------------------------------------
    println!("\n[TEST 1: WRITING TO DISK]");

    // Parcode
    ALLOCATOR.reset();
    let t_start = Instant::now();
    Parcode::save(path_par.path(), &world_data)?;
    let t_par_write = t_start.elapsed();
    let mem_par_write = ALLOCATOR.peak();
    let size_par = path_par.path().metadata()?.len();

    // Bincode
    ALLOCATOR.reset();
    let t_start = Instant::now();
    {
        let mut w = BufWriter::new(File::create(path_bin.path())?);
        bincode::serde::encode_into_std_write(&world_data, &mut w, bincode::config::standard())
            .unwrap();
    }
    let t_bin_write = t_start.elapsed();
    let mem_bin_write = ALLOCATOR.peak();
    let size_bin = path_bin.path().metadata()?.len();

    print_metric("Write Time", t_par_write, t_bin_write);
    print_ram("Write RAM", mem_par_write, mem_bin_write);
    print_size("Disk Size", size_par, size_bin);

    // ------------------------------------------------------------------------
    // TEST 2: COLD START (Open File & Parse Metadata)
    // ------------------------------------------------------------------------
    println!("\n[TEST 2: COLD START (Ready to Read)]");

    // Parcode: Lazy Read (Solo lee Header + Root Chunk)
    ALLOCATOR.reset();
    let t_start = Instant::now();
    let reader = ParcodeReader::open(path_par.path())?;
    let lazy_world = reader.read_lazy::<WorldState>()?;
    let t_par_open = t_start.elapsed();
    let mem_par_open = ALLOCATOR.peak();

    // Bincode: Must deserialize EVERYTHING to be usable
    ALLOCATOR.reset();
    let t_start = Instant::now();
    let file = File::open(path_bin.path())?;
    let mut br = BufReader::new(file);
    let full_world: WorldState =
        bincode::serde::decode_from_std_read(&mut br, bincode::config::standard()).unwrap();
    let t_bin_open = t_start.elapsed();
    let mem_bin_open = ALLOCATOR.peak();

    print_metric("Time to Ready", t_par_open, t_bin_open); // Expect massive win for Parcode
    print_ram("RAM to Ready", mem_par_open, mem_bin_open);

    // ------------------------------------------------------------------------
    // TEST 3: DEEP SURGICAL FETCH
    // Target: World -> Region[5] -> Zone[20] -> TerrainData (First byte)
    // ------------------------------------------------------------------------
    println!("\n[TEST 3: DEEP SURGICAL FETCH]");

    // Parcode: Navigate Graph
    ALLOCATOR.reset();
    let t_start = Instant::now();
    // 1. Get Region 5 Proxy (LazyCollection::get devuelve Region)
    let region = lazy_world.regions.get(5)?;
    // 2. Region struct has 'zones' field which is LazyCollection<Zone> created via macro?
    // Wait, `Region` struct is: #[parcode(chunkable)] zones: Vec<Zone>.
    // The macro generated `RegionLazy` containing `zones: LazyCollection<Zone>`.
    // BUT `lazy_world.regions.get(5)` returns a `Region` (the full struct), NOT `RegionLazy`.
    // Why? Because `LazyCollection::get` returns `T` (deserialized).
    //
    // TO ACHIEVE TRUE DEEP LAZY:
    // We should not deserialize `Region`. We want a `RegionLazy`.
    // Current Implementation limitation: `LazyCollection<T>` deserializes T.
    // If T is `Region`, it deserializes `Region`.
    // Since `Region` has `zones` as chunkable, deserializing `Region` DOES NOT load `zones` data!
    // It only loads the `ChildRef` pointing to `zones`.
    // So `Region` is lightweight! It contains a `Vec` which the visitor implementation
    // reconstructs?
    // NO. If `Region` implements `ParcodeNative`, `from_node` will trigger.
    // Inside `from_node` for Region, `zones` (remote) calls `Vec::from_node`.
    // `Vec::from_node` loads the whole vector.
    //
    // AHA! To support deep lazy traversal, `LazyCollection::get` should ideally allow returning
    // a Lazy version if available. But currently returns `T`.
    //
    // HOWEVER: Even if it loads `Region`, does it load the content of `zones`?
    // Yes, standard `ParcodeNative` is recursive eager.
    //
    // TRUCO: Parcode is optimized. `Region` struct contains `Vec<Zone>`.
    // If we load `Region`, we load `Vec<Zone>`. `Zone` contains `Vec<u8>`.
    // Loading `Vec<Zone>` loads all zones.
    // This implies Test 3 will load Region 5 completely.
    // Compared to Bincode (Loaded EVERYTHING), it's still a win.

    let region_val = lazy_world.regions.get(5)?; // Loads Region 5 + All its Zones
    let zone_val = &region_val.zones[20];
    let byte = zone_val.terrain_data[0];

    let t_par_deep = t_start.elapsed();
    let mem_par_deep = ALLOCATOR.peak();
    black_box(byte);

    // Bincode: Already loaded in memory (zero time now, but paid huge upfront cost).
    // To be fair, we compare "Time to fetch specific item from cold disk".
    // Bincode Time = Open Time (Test 2) + Access Time (0).
    let t_bin_deep = t_bin_open;

    print_metric("Fetch Time", t_par_deep, t_bin_deep);
    print_ram("Fetch RAM", mem_par_deep, mem_bin_open);

    // ------------------------------------------------------------------------
    // TEST 4: MAP LOOKUP (Random Access)
    // Target: User #88888
    // ------------------------------------------------------------------------
    println!("\n[TEST 4: MAP LOOKUP (User #88888)]");

    // Parcode: Hash -> Bucket -> Scan
    ALLOCATOR.reset();
    let t_start = Instant::now();
    let user = lazy_world.users.get(&88888)?.expect("User not found");
    let t_par_map = t_start.elapsed();
    let mem_par_map = ALLOCATOR.peak();
    black_box(user);

    // Bincode: Memory lookup (Fast) but heavily taxed by initial load.
    // Again, comparing Cold Access Time.
    let t_bin_map = t_bin_open;

    print_metric("Lookup Time", t_par_map, t_bin_map);
    print_ram("Lookup RAM", mem_par_map, mem_bin_open);

    Ok(())
}

// --- UTILS ---

fn print_metric(label: &str, par: std::time::Duration, bin: std::time::Duration) {
    let p_ms = par.as_micros() as f64 / 1000.0;
    let b_ms = bin.as_micros() as f64 / 1000.0;
    let ratio = b_ms / p_ms;
    println!(
        "{:<15} | Parcode: {:>8.2} ms | Bincode: {:>8.2} ms | Speedup: {:>5.1}x",
        label, p_ms, b_ms, ratio
    );
}

fn print_ram(label: &str, par: usize, bin: usize) {
    let p_mb = par as f64 / 1048576.0;
    let b_mb = bin as f64 / 1048576.0;
    let savings = (1.0 - (p_mb / b_mb)) * 100.0;
    println!(
        "{:<15} | Parcode: {:>8.2} MB | Bincode: {:>8.2} MB | Savings: {:>5.1}%",
        label, p_mb, b_mb, savings
    );
}

fn print_size(label: &str, par: u64, bin: u64) {
    let p_mb = par as f64 / 1048576.0;
    let b_mb = bin as f64 / 1048576.0;
    let overhead = ((p_mb / b_mb) - 1.0) * 100.0;
    println!(
        "{:<15} | Parcode: {:>8.2} MB | Bincode: {:>8.2} MB | Overhead: {:>5.1}%",
        label, p_mb, b_mb, overhead
    );
}

fn black_box<T>(dummy: T) -> T {
    std::hint::black_box(dummy)
}

===== examples\lazy_loading.rs =====
// examples/lazy_loading.rs

//! Demuestra el acceso perezoso y granular a estructuras complejas.
//! Run: cargo run --example lazy_loading
#![allow(missing_docs)]

use parcode::{Parcode, ParcodeObject, ParcodeReader};
use serde::{Deserialize, Serialize};
use std::time::Instant;
use tempfile::NamedTempFile;

/// Asset with chunkable data
#[derive(Serialize, Deserialize, Clone, ParcodeObject)]
struct BigAsset {
    /// Asset ID
    id: u32,
    /// Raw data
    #[parcode(chunkable)]
    data: Vec<u8>,
}

#[derive(Serialize, Deserialize, Clone, ParcodeObject)]
struct GameWorld {
    /// Name of the world
    world_name: String,

    /// Skybox asset
    #[parcode(chunkable)]
    skybox: BigAsset,

    /// Terrain asset
    #[parcode(chunkable)]
    terrain: BigAsset,
}

fn main() -> parcode::Result<()> {
    println!("--- Parcode Lazy Loading Example ---");

    // 1. Generar Datos
    let asset_size = 50 * 1024 * 1024; // 50MB
    println!(
        "Generating world with two {} MB assets...",
        asset_size / 1024 / 1024
    );

    let world = GameWorld {
        world_name: "Azeroth".into(),
        skybox: BigAsset {
            id: 1,
            data: vec![1u8; asset_size],
        },
        terrain: BigAsset {
            id: 2,
            data: vec![2u8; asset_size],
        },
    };

    let file = NamedTempFile::new().expect("Failed to create temp file");

    println!("Saving world...");
    let start = Instant::now();
    Parcode::save(file.path(), &world)?;
    println!("Saved in {:.2?}", start.elapsed());

    // 2. Lectura Lazy
    println!("\n--- Lazy Access ---");
    let reader = ParcodeReader::open(file.path())?;

    let start_lazy = Instant::now();
    let lazy_world = reader.read_lazy::<GameWorld>()?;
    println!("Lazy Metadata Loaded in {:.2?}", start_lazy.elapsed());

    // Acceso a metadatos locales (Instantáneo)
    println!("World Name: {}", lazy_world.world_name);

    // Navegación profunda (Instantáneo, solo lee headers)
    println!("Skybox ID: {}", lazy_world.skybox.id);
    println!("Terrain ID: {}", lazy_world.terrain.id);

    // Carga selectiva
    println!("Loading ONLY Skybox data...");
    let load_start = Instant::now();
    let sky_data = lazy_world.skybox.data.load()?;
    println!(
        "Loaded Skybox ({} bytes) in {:.2?}",
        sky_data.len(),
        load_start.elapsed()
    );

    println!("Done. Notice we never loaded Terrain data!");

    Ok(())
}

===== examples\map_benchmark.rs =====
// examples/map_benchmark.rs
//! Comprehensive Benchmark: HashMap Optimization vs Standard Serialization.
//! Run: cargo run --example map_benchmark --release --features lz4_flex

#![allow(missing_docs)]
#![allow(unsafe_code)]

use parcode::{Parcode, ParcodeObject, ParcodeReader};
use serde::{Deserialize, Serialize};
use std::alloc::{GlobalAlloc, Layout, System};
use std::collections::HashMap;
use std::fs::File;
use std::io::{BufReader, BufWriter};
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::Instant;
use tempfile::NamedTempFile;

// ============================================================================
// 1. MEMORY PROFILER
// ============================================================================

struct ProfilingAllocator<A: GlobalAlloc> {
    inner: A,
    allocated: AtomicUsize,
    peak: AtomicUsize,
}

impl<A: GlobalAlloc> ProfilingAllocator<A> {
    const fn new(inner: A) -> Self {
        Self {
            inner,
            allocated: AtomicUsize::new(0),
            peak: AtomicUsize::new(0),
        }
    }
    fn reset(&self) {
        self.allocated.store(0, Ordering::SeqCst);
        self.peak.store(0, Ordering::SeqCst);
    }
    fn peak(&self) -> usize {
        self.peak.load(Ordering::SeqCst)
    }
}

unsafe impl<A: GlobalAlloc> GlobalAlloc for ProfilingAllocator<A> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let ptr = unsafe { self.inner.alloc(layout) };
        if !ptr.is_null() {
            let current = self.allocated.fetch_add(layout.size(), Ordering::SeqCst) + layout.size();
            self.peak.fetch_max(current, Ordering::SeqCst);
        }
        ptr
    }
    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        self.allocated.fetch_sub(layout.size(), Ordering::SeqCst);
        unsafe { self.inner.dealloc(ptr, layout) };
    }
}

#[global_allocator]
static ALLOCATOR: ProfilingAllocator<System> = ProfilingAllocator::new(System);

// ============================================================================
// 2. DATA MODELS
// ============================================================================

// Mode A: Optimized Map (Bucket Sharding + Micro-Index)
#[derive(Serialize, Deserialize, ParcodeObject)]
struct OptimizedMap {
    #[parcode(map)]
    data: HashMap<u64, String>,
}

// Mode B: Standard Parcode (Single Chunk Blob)
#[derive(Serialize, Deserialize, ParcodeObject)]
struct StandardMap {
    #[parcode(chunkable)]
    data: HashMap<u64, String>,
}

// ============================================================================
// 3. BENCHMARK SUITE
// ============================================================================

fn main() -> parcode::Result<()> {
    println!("\n=== PARCODE MAP OPTIMIZATION AUDIT ===\n");

    let count = 500_000;
    print!("Generating dataset ({} items)... ", count);
    ALLOCATOR.reset();
    let mut map = HashMap::with_capacity(count);
    for i in 0..count {
        map.insert(
            i as u64,
            format!("payload_value_{}_extended_string_data", i),
        );
    }
    println!(
        "Done. Base RAM: {:.2} MB",
        ALLOCATOR.peak() as f64 / 1024.0 / 1024.0
    );

    let opt_data = OptimizedMap { data: map.clone() };
    let std_data = StandardMap { data: map.clone() };

    let file_opt = NamedTempFile::new().unwrap();
    let file_std = NamedTempFile::new().unwrap();
    let file_bin = NamedTempFile::new().unwrap();

    // ------------------------------------------------------------------------
    // PHASE 1: WRITE PERFORMANCE
    // ------------------------------------------------------------------------
    println!("\n[PHASE 1: WRITE]");

    // A. Optimized
    ALLOCATOR.reset();
    let start = Instant::now();
    Parcode::save(file_opt.path(), &opt_data)?;
    let dur = start.elapsed();
    let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
    let size_opt = file_opt.path().metadata()?.len() as f64 / 1_048_576.0;
    println!(
        "Parcode (Optimized): Time: {:.2?}, RAM Peak: {:.2} MB, Disk: {:.2} MB",
        dur, peak, size_opt
    );

    // B. Standard (Blob)
    ALLOCATOR.reset();
    let start = Instant::now();
    Parcode::save(file_std.path(), &std_data)?;
    let dur = start.elapsed();
    let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
    let size_std = file_std.path().metadata()?.len() as f64 / 1_048_576.0;
    println!(
        "Parcode (Standard):  Time: {:.2?}, RAM Peak: {:.2} MB, Disk: {:.2} MB",
        dur, peak, size_std
    );

    // C. Bincode
    ALLOCATOR.reset();
    let start = Instant::now();
    {
        let mut writer = BufWriter::new(File::create(file_bin.path())?);
        bincode::serde::encode_into_std_write(&map, &mut writer, bincode::config::standard())
            .unwrap();
    }
    let dur = start.elapsed();
    let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
    let size_bin = file_bin.path().metadata()?.len() as f64 / 1_048_576.0;
    println!(
        "Bincode (Native):    Time: {:.2?}, RAM Peak: {:.2} MB, Disk: {:.2} MB",
        dur, peak, size_bin
    );

    // ------------------------------------------------------------------------
    // PHASE 2: RANDOM ACCESS (1000 Lookups)
    // ------------------------------------------------------------------------
    println!("\n[PHASE 2: RANDOM ACCESS (1000 lookups)]");

    // A. Optimized (Lazy)
    ALLOCATOR.reset();
    let reader = ParcodeReader::open(file_opt.path())?;
    let lazy = reader.read_lazy::<OptimizedMap>()?;
    let start = Instant::now();
    let mut hits = 0;
    for i in 0..1000 {
        let key = (i * 12345) as u64 % count as u64;
        if lazy.data.get(&key)?.is_some() {
            hits += 1;
        }
    }
    let dur = start.elapsed();
    let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
    println!(
        "Parcode (Optimized): Time: {:.2?} ({:.2} µs/op), RAM Peak: {:.2} MB",
        dur,
        dur.as_micros() as f64 / 1000.0,
        peak
    );
    assert!(hits > 0);

    // B. Standard (Full Load required)
    // No podemos hacer random access sin cargar todo el blob
    ALLOCATOR.reset();
    let reader = ParcodeReader::open(file_std.path())?;
    let lazy = reader.read_lazy::<StandardMap>()?;
    let start = Instant::now();
    // Simulamos: Cargar todo 1 vez, luego 1000 lookups en memoria
    let loaded_map = lazy.data.load()?; // Coste masivo aquí
    for i in 0..1000 {
        let key = (i * 12345) as u64 % count as u64;
        black_box(loaded_map.get(&key));
    }
    let dur = start.elapsed();
    let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
    println!(
        "Parcode (Standard):  Time: {:.2?}, RAM Peak: {:.2} MB (Includes Full Load)",
        dur, peak
    );

    // C. Bincode (Full Load required)
    ALLOCATOR.reset();
    let start = Instant::now();
    let file = File::open(file_bin.path())?;
    let mut reader = BufReader::new(file);
    let loaded_map: HashMap<u64, String> =
        bincode::serde::decode_from_std_read(&mut reader, bincode::config::standard()).unwrap();
    for i in 0..1000 {
        let key = (i * 12345) as u64 % count as u64;
        black_box(loaded_map.get(&key));
    }
    let dur = start.elapsed();
    let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
    println!(
        "Bincode (Native):    Time: {:.2?}, RAM Peak: {:.2} MB (Includes Full Load)",
        dur, peak
    );

    Ok(())
}

fn black_box<T>(dummy: T) -> T {
    std::hint::black_box(dummy)
}

===== examples\memory_test.rs =====
//! Memory Profiling & Performance Example for Parcode V3.
//!
//! Run with: cargo run --example `memory_test` --release --features lz4_flex

#![allow(missing_docs)]
#![allow(unsafe_code)]

use parcode::{
    Parcode, ParcodeError, ParcodeReader, Result,
    format::ChildRef,
    graph::{ChunkId, JobConfig, SerializationJob, TaskGraph},
    reader::{ChunkNode, ParcodeNative},
    visitor::ParcodeVisitor,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs::File;
use std::hint::black_box;
use std::io::{BufReader, BufWriter};
use std::{
    alloc::{GlobalAlloc, Layout, System},
    sync::atomic::{AtomicUsize, Ordering},
    time::Instant,
};
use tempfile::tempdir;

// ============================================================================
// 1. MEMORY ALLOCATOR PROFILER
// ============================================================================

/// A wrapper around the System allocator that tracks memory usage.
struct ProfilingAllocator<A: GlobalAlloc> {
    inner: A,
    allocated: AtomicUsize,
    peak: AtomicUsize,
}

impl<A: GlobalAlloc> ProfilingAllocator<A> {
    const fn new(inner: A) -> Self {
        Self {
            inner,
            allocated: AtomicUsize::new(0),
            peak: AtomicUsize::new(0),
        }
    }

    fn reset(&self) {
        self.allocated.store(0, Ordering::SeqCst);
        self.peak.store(0, Ordering::SeqCst);
    }

    fn peak(&self) -> usize {
        self.peak.load(Ordering::SeqCst)
    }

    fn current(&self) -> usize {
        self.allocated.load(Ordering::SeqCst)
    }
}

unsafe impl<A: GlobalAlloc> GlobalAlloc for ProfilingAllocator<A> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let ptr: *mut u8;
        unsafe {
            ptr = self.inner.alloc(layout);
        }
        if !ptr.is_null() {
            let current = self.allocated.fetch_add(layout.size(), Ordering::SeqCst) + layout.size();
            self.peak.fetch_max(current, Ordering::SeqCst);
        }
        ptr
    }

    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        self.allocated.fetch_sub(layout.size(), Ordering::SeqCst);
        unsafe {
            self.inner.dealloc(ptr, layout);
        }
    }
}

#[global_allocator]
static ALLOCATOR: ProfilingAllocator<System> = ProfilingAllocator::new(System);

// ============================================================================
// 2. DATA STRUCTURES
// ============================================================================

#[derive(Serialize, Deserialize, Clone, PartialEq, Debug)]
struct ComplexItem {
    id: u64,
    name: String,
    // Simulates binary blobs (textures, buffers)
    #[serde(with = "serde_bytes")]
    payload: Vec<u8>,
    tags: HashMap<String, String>,
}

// --- MANUAL IMPL (Simulating what the Macro will generate) ---

impl ParcodeVisitor for ComplexItem {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        // Logic: If root or inside a collection, create a node.
        // This simulates a "Heavy Leaf" that doesn't split further.
        if parent_id.is_none() {
            let job = self.create_job(config_override);
            graph.add_node(job);
        }
    }

    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        let base = Box::new(self.clone());
        // Apply config override (e.g. Compression) if provided by parent/macro
        if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(base, cfg))
        } else {
            base
        }
    }
}

impl SerializationJob<'_> for ComplexItem {
    fn execute(&self, _: &[ChildRef]) -> Result<Vec<u8>> {
        // Low-level serialization using Bincode
        bincode::serde::encode_to_vec(self, bincode::config::standard())
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }
    fn estimated_size(&self) -> usize {
        self.payload.len() + 200
    }
}

impl ParcodeNative for ComplexItem {
    fn from_node(node: &ChunkNode<'_>) -> Result<Self> {
        node.decode::<Self>()
    }
}

// --- CONFIGURATION WRAPPER ---
// Simulates usage of #[parcode(compression="lz4")] on a field.
// This wrapper forces LZ4 on the underlying data.

struct Lz4Compressed<T>(pub T);

impl<T: ParcodeVisitor> ParcodeVisitor for Lz4Compressed<T> {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        _config_override: Option<JobConfig>,
    ) {
        // FORCE LZ4 Config (ID 1)
        let lz4_config = JobConfig {
            compression_id: 1,
            is_map: false,
        };

        // Delegate to inner
        self.0.visit(graph, parent_id, Some(lz4_config));
    }

    fn create_job<'a>(&'a self, _config: Option<JobConfig>) -> Box<dyn SerializationJob<'a> + 'a> {
        panic!("Wrapper should not create job directly in this example usage");
    }
}

// ============================================================================
// 3. GENERATOR
// ============================================================================

fn generate_dataset(count: usize) -> Vec<ComplexItem> {
    print!(" -> Generating {} items... ", count);
    let start = Instant::now();
    let data = (0..count)
        .map(|i| {
            // Varied payload size to test fragmentation
            let size = (i % 1024) + 128;
            ComplexItem {
                id: i as u64,
                name: format!("asset_{:08}", i),
                payload: vec![((i * 3) % 255) as u8; size],
                tags: (0..3)
                    .map(|j| (format!("meta_{}", j), format!("val_{}", i + j)))
                    .collect(),
            }
        })
        .collect();
    println!("Done in {:.2?}", start.elapsed());
    data
}

// ============================================================================
// 4. MAIN BENCHMARK
// ============================================================================

fn main() -> anyhow::Result<()> {
    const ITEM_COUNT: usize = 200_000;
    // Approx size: 200k * ~600 bytes = ~120 MB raw data

    let dir = tempdir()?;
    let path_parcode_raw = dir.path().join("data_raw.par");
    let path_parcode_lz4 = dir.path().join("data_lz4.par");
    let path_bincode = dir.path().join("data.bin");

    println!("\n=== PARCODE V3 MEMORY & PERFORMANCE PROFILING ===\n");

    // 1. SETUP
    ALLOCATOR.reset();
    let data = generate_dataset(ITEM_COUNT);
    let gen_mem = ALLOCATOR.current();
    println!(
        " -> Dataset Memory Footprint: {:.2} MB\n",
        gen_mem as f64 / 1024.0 / 1024.0
    );

    // ------------------------------------------------------------------------
    // PHASE 1: WRITE
    // ------------------------------------------------------------------------
    println!("--- PHASE 1: WRITE ---");

    // A. Parcode (Default / No Compression)
    {
        ALLOCATOR.reset();
        let start = Instant::now();
        Parcode::save(&path_parcode_raw, &data)?;
        let dur = start.elapsed();
        let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
        let size = std::fs::metadata(&path_parcode_raw)?.len() as f64 / 1_048_576.0;

        println!(
            "[Parcode Raw]    Time: {:.2?}, Peak RAM: {:.2} MB, Disk: {:.2} MB",
            dur, peak, size
        );
    }

    // B. Parcode (Simulated LZ4 Config via Wrapper)
    {
        // Wrap the data to force LZ4 injection
        let wrapped_data = Lz4Compressed(&data);

        ALLOCATOR.reset();
        let start = Instant::now();

        // We save the wrapper. The wrapper's visit() forces LZ4 on the Vec.
        // Note: Parcode::save takes &T.
        Parcode::save(&path_parcode_lz4, &wrapped_data)?;

        let dur = start.elapsed();
        let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
        let size = std::fs::metadata(&path_parcode_lz4)?.len() as f64 / 1_048_576.0;

        println!(
            "[Parcode LZ4]    Time: {:.2?}, Peak RAM: {:.2} MB, Disk: {:.2} MB",
            dur, peak, size
        );
    }

    // C. Bincode (Baseline)
    {
        ALLOCATOR.reset();
        let start = Instant::now();
        let file = File::create(&path_bincode)?;
        let mut writer = BufWriter::new(file);
        bincode::serde::encode_into_std_write(&data, &mut writer, bincode::config::standard())?;
        let dur = start.elapsed();
        let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
        let size = std::fs::metadata(&path_bincode)?.len() as f64 / 1_048_576.0;

        println!(
            "[Bincode Baseline] Time: {:.2?}, Peak RAM: {:.2} MB, Disk: {:.2} MB",
            dur, peak, size
        );
    }

    // ------------------------------------------------------------------------
    // PHASE 2: FULL READ (Reconstruction)
    // ------------------------------------------------------------------------
    println!("\n--- PHASE 2: FULL PARALLEL READ ---");

    // A. Parcode
    {
        ALLOCATOR.reset();
        let start = Instant::now();
        let loaded: Vec<ComplexItem> = Parcode::read(&path_parcode_raw)?;
        black_box(loaded.len());
        let dur = start.elapsed();
        let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;

        println!(
            "[Parcode Stitching] Time: {:.2?}, Peak RAM: {:.2} MB (Zero-Copy Assembly)",
            dur, peak
        );
        assert_eq!(loaded.len(), ITEM_COUNT);
    }

    // B. Bincode
    {
        ALLOCATOR.reset();
        let start = Instant::now();
        let file = File::open(&path_bincode)?;
        let mut reader = BufReader::new(file);
        let loaded: Vec<ComplexItem> =
            bincode::serde::decode_from_std_read(&mut reader, bincode::config::standard())?;
        black_box(loaded.len());
        let dur = start.elapsed();
        let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;

        println!(
            "[Bincode Standard]  Time: {:.2?}, Peak RAM: {:.2} MB",
            dur, peak
        );
    }

    // ------------------------------------------------------------------------
    // PHASE 3: STREAMING (Iterative Read)
    // ------------------------------------------------------------------------
    println!("\n--- PHASE 3: STREAMING (Low Memory) ---");

    {
        ALLOCATOR.reset();
        let start = Instant::now();
        let reader = ParcodeReader::open(&path_parcode_lz4)?; // Reading the compressed one!
        let root = reader.root()?;

        let mut count = 0;
        // Using the low-level iterator
        for item in root.iter::<ComplexItem>()? {
            let obj = item?;
            count += 1;
            black_box(obj.id);
        }

        let dur = start.elapsed();
        let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
        println!(
            "[Parcode Iterator]  Time: {:.2?}, Peak RAM: {:.2} MB (LZ4 Decompression on fly)",
            dur, peak
        );
        assert_eq!(count, ITEM_COUNT);
    }

    // ------------------------------------------------------------------------
    // PHASE 4: RANDOM ACCESS
    // ------------------------------------------------------------------------
    println!("\n--- PHASE 4: RANDOM ACCESS ---");

    {
        ALLOCATOR.reset();
        let start = Instant::now();
        let reader = ParcodeReader::open(&path_parcode_raw)?;
        let root = reader.root()?;

        // Access 10 random items spread across the dataset
        for i in 0..10 {
            let idx = (i * 12345) % ITEM_COUNT;
            let item: ComplexItem = root.get(idx)?;
            assert_eq!(item.id, idx as u64);
        }

        let dur = start.elapsed();
        let peak = ALLOCATOR.peak() as f64 / 1_048_576.0;
        println!(
            "[Parcode Random]    Time: {:.2?}, Peak RAM: {:.2} MB (10 lookups)",
            dur, peak
        );
    }

    println!("\nDone.");
    Ok(())
}

===== parcode-derive\src\lib.rs =====
// parcode-derive/src/lib.rs

//! # Parcode Derive Macros
//!
//! This crate provides the procedural macros for `parcode`. It automates the implementation
//! of the `ParcodeVisitor`, `SerializationJob`, `ParcodeNative`, and the Lazy Mirror.
//!
//! Compatible with `syn 2.0`.

use proc_macro::TokenStream;
use quote::quote;
use syn::{Attribute, Data, DeriveInput, LitStr, parse_macro_input};

/// Derives `ParcodeVisitor`, `SerializationJob`, `ParcodeNative` and `ParcodeLazyRef`.
#[proc_macro_derive(ParcodeObject, attributes(parcode))]
pub fn derive_parcode_object(input: TokenStream) -> TokenStream {
    let input = parse_macro_input!(input as DeriveInput);
    let name = input.ident;

    let data_struct = match input.data {
        Data::Struct(ds) => ds,
        _ => {
            return syn::Error::new(name.span(), "ParcodeObject only supports structs")
                .to_compile_error()
                .into();
        }
    };

    let mut locals = Vec::new();
    let mut remotes = Vec::new();

    for field in data_struct.fields {
        // Parse attributes.
        let (is_chunkable, compression_id, is_map) = match parse_attributes(&field.attrs) {
            Ok(res) => res,
            Err(e) => return e.to_compile_error().into(),
        };

        // CORRECCIÓN: Si es 'map', implícitamente es chunkable (es un nodo hijo).
        if is_chunkable || is_map {
            remotes.push(RemoteField {
                ident: field.ident.clone().unwrap(),
                ty: field.ty.clone(),
                compression_id,
                is_map, // NUEVO CAMPO
            });
        } else {
            locals.push(LocalField {
                ident: field.ident.clone().unwrap(),
                ty: field.ty.clone(),
            });
        }
    }

    let impl_visitor = generate_visitor(&name, &remotes, &locals);
    let impl_job = generate_serialization_job(&name, &locals);
    let impl_native = generate_native_reader(&name, &locals, &remotes);
    let impl_lazy = generate_lazy_mirror(&name, &locals, &remotes);

    let expanded = quote! {
        #impl_visitor
        #impl_job
        #impl_native
        #impl_lazy
    };

    TokenStream::from(expanded)
}

// --- Internal Data Structures (Same as before) ---
struct LocalField {
    ident: syn::Ident,
    ty: syn::Type,
}
struct RemoteField {
    ident: syn::Ident,
    ty: syn::Type,
    compression_id: u8,
    is_map: bool,
}

/// Parses attributes. Returns (is_chunkable, compression_id, is_map).
fn parse_attributes(attrs: &[Attribute]) -> syn::Result<(bool, u8, bool)> {
    let mut is_chunkable = false;
    let mut is_map = false;
    let mut compression_id = 0;

    for attr in attrs {
        if attr.path().is_ident("parcode") {
            attr.parse_nested_meta(|meta| {
                if meta.path.is_ident("chunkable") {
                    is_chunkable = true;
                    return Ok(());
                }

                // NUEVO: Soporte para #[parcode(map)]
                if meta.path.is_ident("map") {
                    is_map = true;
                    return Ok(());
                }

                if meta.path.is_ident("compression") {
                    let value = meta.value()?;
                    let s: LitStr = value.parse()?;
                    compression_id = match s.value().to_lowercase().as_str() {
                        "lz4" => 1,
                        "zstd" => 2,
                        "none" => 0,
                        _ => return Err(meta.error("Unknown compression algorithm")),
                    };
                    return Ok(());
                }
                Err(meta
                    .error("Unknown parcode attribute key. Supported: chunkable, map, compression"))
            })?;
        }
    }
    Ok((is_chunkable, compression_id, is_map))
}

// --- Generator: ParcodeVisitor ---
fn generate_visitor(
    name: &syn::Ident,
    remotes: &[RemoteField],
    _locals: &[LocalField],
) -> proc_macro2::TokenStream {
    let visit_children = remotes.iter().map(|f| {
        let fname = &f.ident;
        let cid = f.compression_id;
        let is_map = f.is_map;

        // Construimos el JobConfig dinámicamente
        let config_expr = if cid > 0 || is_map {
            quote! {
                Some(parcode::graph::JobConfig {
                    compression_id: #cid,
                    is_map: #is_map
                })
            }
        } else {
            quote! { None }
        };

        quote! { self.#fname.visit(graph, Some(my_id), #config_expr); }
    });

    quote! {
        impl parcode::visitor::ParcodeVisitor for #name {
            fn visit<'a>(&'a self, graph: &mut parcode::graph::TaskGraph<'a>, parent_id: Option<parcode::graph::ChunkId>, config_override: Option<parcode::graph::JobConfig>) {
                let job = self.create_job(config_override);
                let my_id = graph.add_node(job);
                if let Some(pid) = parent_id { graph.link_parent_child(pid, my_id); }
                #(#visit_children)*
            }
            fn create_job<'a>(&'a self, config_override: Option<parcode::graph::JobConfig>) -> Box<dyn parcode::graph::SerializationJob<'a> + 'a> {
                let base_job = Box::new(self.clone());
                if let Some(cfg) = config_override { Box::new(parcode::rt::ConfiguredJob::new(base_job, cfg)) } else { base_job }
            }
        }
    }
}

// --- Generator: SerializationJob ---
fn generate_serialization_job(
    name: &syn::Ident,
    locals: &[LocalField],
) -> proc_macro2::TokenStream {
    let serialize_stmts = locals.iter().map(|f| {
        let fname = &f.ident;
        quote! {
            parcode::internal::bincode::serde::encode_into_std_write(
                &self.#fname, &mut writer, parcode::internal::bincode::config::standard()
            ).map_err(|e| parcode::ParcodeError::Serialization(e.to_string()))?;
        }
    });

    quote! {
        impl<'a> parcode::graph::SerializationJob<'a> for #name {
            fn execute(&self, _children_refs: &[parcode::format::ChildRef]) -> parcode::Result<Vec<u8>> {
                let mut buffer = Vec::new();
                let mut writer = std::io::BufWriter::new(&mut buffer);
                #(#serialize_stmts)*
                use std::io::Write;
                writer.flush()?;
                drop(writer);
                Ok(buffer)
            }
            fn estimated_size(&self) -> usize { std::mem::size_of::<Self>() }
        }
    }
}

// --- Generator: ParcodeNative (Same as before) ---
fn generate_native_reader(
    name: &syn::Ident,
    locals: &[LocalField],
    remotes: &[RemoteField],
) -> proc_macro2::TokenStream {
    let read_locals = locals.iter().map(|f| {
        let fname = &f.ident;
        let fty = &f.ty;
        quote! {
            let #fname: #fty = parcode::internal::bincode::serde::decode_from_std_read(
                &mut reader, parcode::internal::bincode::config::standard()
            ).map_err(|e| parcode::ParcodeError::Serialization(e.to_string()))?;
        }
    });

    let read_remotes = remotes.iter().map(|f| {
        let fname = &f.ident;
        let fty = &f.ty;
        quote! {
            let child_node = child_iter.next().ok_or_else(|| parcode::ParcodeError::Format(format!("Missing child for '{}'", stringify!(#fname))))?;
            let #fname: #fty = parcode::reader::ParcodeNative::from_node(&child_node)?;
        }
    });

    let mut field_names = Vec::new();
    for f in locals {
        field_names.push(&f.ident);
    }
    for f in remotes {
        field_names.push(&f.ident);
    }

    quote! {
        impl parcode::reader::ParcodeNative for #name {
            fn from_node(node: &parcode::reader::ChunkNode<'_>) -> parcode::Result<Self> {
                let payload = node.read_raw()?;
                let mut reader = std::io::Cursor::new(payload);
                #(#read_locals)*
                let children = node.children()?;
                let mut child_iter = children.into_iter();
                #(#read_remotes)*
                Ok(Self { #(#field_names),* })
            }
        }
    }
}

// --- NEW GENERATOR: LAZY MIRROR ---

fn generate_lazy_mirror(
    name: &syn::Ident,
    locals: &[LocalField],
    remotes: &[RemoteField],
) -> proc_macro2::TokenStream {
    let lazy_name = syn::Ident::new(&format!("{}Lazy", name), name.span());

    let lazy_fields = locals
        .iter()
        .map(|f| {
            let n = &f.ident;
            let t = &f.ty;
            quote! { pub #n: #t }
        })
        .chain(remotes.iter().map(|f| {
            let n = &f.ident;
            let t = &f.ty;

            if f.is_map {
                // CORRECCIÓN VITAL: Si es mapa, usamos ParcodeMapPromise explícitamente.
                // Necesitamos extraer K y V del tipo HashMap<K,V>.
                // Parsear generics en syn es complejo.
                // ALTERNATIVA MEJOR: Usamos el trait ParcodeLazyRef que definimos para HashMap.
                // HashMap implementa Lazy = ParcodeMapPromise.
                // Así que la lógica es IDÉNTICA al caso estándar: <T>::Lazy.
                quote! { pub #n: <#t as parcode::rt::ParcodeLazyRef<'a>>::Lazy }
            } else {
                quote! { pub #n: <#t as parcode::rt::ParcodeLazyRef<'a>>::Lazy }
            }
        }));

    let assign_remotes_stmts = remotes.iter().map(|f| {
        let n = &f.ident;
        let t = &f.ty;
        quote! {
            let child = child_iter.next().ok_or(parcode::ParcodeError::Format("Missing child".into()))?;
            let #n = <#t as parcode::rt::ParcodeLazyRef<'a>>::create_lazy(child)?;
        }
    });

    let mut field_names = Vec::new();
    for f in locals {
        field_names.push(&f.ident);
    }
    for f in remotes {
        field_names.push(&f.ident);
    }

    let read_locals_stmts = locals.iter().map(|f| {
        let n = &f.ident;
        let t = &f.ty;
        quote! {
            let #n: #t = parcode::internal::bincode::serde::decode_from_std_read(
                &mut reader, parcode::internal::bincode::config::standard()
            ).map_err(|e| parcode::ParcodeError::Serialization(e.to_string()))?;
        }
    });

    quote! {
        #[derive(Debug)]
        pub struct #lazy_name<'a> {
            #(#lazy_fields),*
        }

        impl<'a> parcode::rt::ParcodeLazyRef<'a> for #name {
            type Lazy = #lazy_name<'a>;

            fn create_lazy(node: parcode::reader::ChunkNode<'a>) -> parcode::Result<Self::Lazy> {
                let payload = node.read_raw()?;
                let mut reader = std::io::Cursor::new(payload);
                #(#read_locals_stmts)*

                let children = node.children()?;
                let mut child_iter = children.into_iter();
                #(#assign_remotes_stmts)*

                Ok(#lazy_name {
                    #(#field_names),*
                })
            }
        }
    }
}

===== src\api.rs =====
//! High-level API for Parcode.
//!
//! This module provides the primary entry points for reading and writing Parcode files.
//! It offers a builder-style interface for configuration and simple functions for
//! common operations.

use crate::error::Result;
use crate::executor::execute_graph;
use crate::format::GlobalHeader;
use crate::graph::TaskGraph;
use crate::io::SeqWriter;
use crate::reader::{ParcodeNative, ParcodeReader};
use crate::visitor::ParcodeVisitor;
use std::path::Path;

/// The main entry point for configuring and executing Parcode operations.
///
/// # Example
///
/// ```rust,ignore
/// use parcode::Parcode;
///
/// let data = vec![1, 2, 3];
/// Parcode::save("data.par", &data).unwrap();
/// let loaded: Vec<i32> = Parcode::read("data.par").unwrap();
/// ```
#[derive(Debug, Default)]
pub struct Parcode {
    use_compression: bool,
}

impl Parcode {
    /// Creates a new `Parcode` builder with default settings.
    pub fn builder() -> Self {
        Self::default()
    }

    /// Enables or disables compression.
    ///
    /// If enabled, the default compression algorithm (usually LZ4 if enabled) will be used.
    pub fn compression(mut self, enable: bool) -> Self {
        self.use_compression = enable;
        self
    }

    /// Reads an object from a file, automatically selecting the optimal
    /// reconstruction strategy (Parallel for collections, Sequential for leaves).
    pub fn read<T, P>(path: P) -> Result<T>
    where
        T: ParcodeNative, // Constraint ensures we use the specialized logic
        P: AsRef<Path>,
    {
        let reader = ParcodeReader::open(path)?;
        let root = reader.root()?;

        // Dispatches to Vec::from_node (Parallel) or T::from_node (Simple)
        T::from_node(&root)
    }

    /// Saves an object to a file using default settings.
    ///
    /// This is a convenience wrapper around `write`.
    pub fn save<T, P>(path: P, root_object: &T) -> Result<()>
    where
        T: ParcodeVisitor + Sync,
        P: AsRef<Path>,
    {
        Self::default().write(path, root_object)
    }

    /// Serializes the object graph to disk using Zero-Copy where possible.
    /// The `root_object` must outlive the function call (implied).
    pub fn write<'a, T, P>(&self, path: P, root_object: &'a T) -> Result<()>
    where
        T: ParcodeVisitor + Sync,
        P: AsRef<Path>,
    {
        let path = path.as_ref();
        // The graph is tied to lifetime 'a of root_object
        let mut graph = TaskGraph::<'a>::new();

        root_object.visit(&mut graph, None, None);

        let writer = SeqWriter::create(path)?;
        let registry = crate::compression::CompressorRegistry::new();

        let root_child_ref = execute_graph(&graph, &writer, &registry)?;

        let header = GlobalHeader::new(root_child_ref.offset, root_child_ref.length);
        writer.write_all(&header.to_bytes())?;
        writer.flush()?;

        Ok(())
    }
}

===== src\compression.rs =====
//! Pluggable compression backend.
//!
//! Handles the transformation of raw byte buffers into compressed chunks.
//! This module defines the `Compressor` trait and a registry for managing
//! available compression algorithms.

use crate::error::{ParcodeError, Result};
use std::borrow::Cow;

/// Threshold in bytes. (Removed logic usage, kept for reference or future smart heuristics outside the impl)
#[allow(dead_code)]
const MIN_COMPRESSION_THRESHOLD: usize = 64;

/// Interface for compression algorithms.
///
/// Implementors of this trait provide the logic to compress and decompress
/// byte buffers. Each compressor is identified by a unique ID.
pub trait Compressor: Send + Sync + std::fmt::Debug {
    /// Returns the unique ID stored in the `MetaByte` (Bits 1-3).
    /// 0 is reserved for No-Compression.
    fn id(&self) -> u8;

    /// Compresses the data.
    ///
    /// Returns a `Cow<[u8]>` which may borrow the input if no compression is performed
    /// or if the compressed size is larger than the original.
    fn compress<'a>(&self, data: &'a [u8]) -> Result<Cow<'a, [u8]>>;

    /// Decompresses the data.
    ///
    /// Returns a `Cow<[u8]>` containing the original data.
    fn decompress<'a>(&self, data: &'a [u8]) -> Result<Cow<'a, [u8]>>;

    /// Compresses the data and appends it to the output vector.
    ///
    /// This avoids intermediate allocations by writing directly to the final buffer.
    fn compress_append(&self, data: &[u8], output: &mut Vec<u8>) -> Result<()>;
}

// --- No Compression (Pass-through) ---

/// A compressor that performs no compression (pass-through).
///
/// This is the default strategy (ID 0). It simply passes the data through unchanged.
#[derive(Debug, Clone, Copy)]
pub struct NoCompression;

impl Compressor for NoCompression {
    fn id(&self) -> u8 {
        0
    }

    fn compress<'a>(&self, data: &'a [u8]) -> Result<Cow<'a, [u8]>> {
        // Zero-copy: return reference to input
        Ok(Cow::Borrowed(data))
    }

    fn decompress<'a>(&self, data: &'a [u8]) -> Result<Cow<'a, [u8]>> {
        // Zero-copy: return reference to mmap
        Ok(Cow::Borrowed(data))
    }

    fn compress_append(&self, data: &[u8], output: &mut Vec<u8>) -> Result<()> {
        output.extend_from_slice(data);
        Ok(())
    }
}

// --- LZ4 Implementation ---

#[cfg(feature = "lz4_flex")]
/// A compressor using the LZ4 algorithm.
///
/// This compressor is available when the `lz4_flex` feature is enabled.
/// It uses the `lz4_flex` crate for high-performance compression.
#[derive(Debug, Clone, Copy)]
pub struct Lz4Compressor;

#[cfg(feature = "lz4_flex")]
impl Compressor for Lz4Compressor {
    fn id(&self) -> u8 {
        1
    }

    fn compress<'a>(&self, data: &'a [u8]) -> Result<Cow<'a, [u8]>> {
        let compressed = lz4_flex::compress_prepend_size(data);
        Ok(Cow::Owned(compressed))
    }

    fn decompress<'a>(&self, data: &'a [u8]) -> Result<Cow<'a, [u8]>> {
        let vec = lz4_flex::decompress_size_prepended(data)
            .map_err(|e| ParcodeError::Compression(e.to_string()))?;
        Ok(Cow::Owned(vec))
    }

    fn compress_append(&self, data: &[u8], output: &mut Vec<u8>) -> Result<()> {
        // Reserve space for the worst-case compressed size + 4 bytes for size prefix
        // We can't easily use compress_prepend_size without allocation, so we implement it manually
        // to write directly to output.

        // Format: [u32 LE uncompressed_len] [compressed_data]
        // Wait, lz4_flex::compress_prepend_size uses a specific format.
        // It stores uncompressed size as u32 LE? No, it might be varint or just u32.
        // Checking lz4_flex docs (recalled): it uses u32 little endian.

        let uncompressed_len = data.len() as u32;
        output.extend_from_slice(&uncompressed_len.to_le_bytes());

        let _start_idx = output.len();
        let max_size = lz4_flex::block::get_maximum_output_size(data.len());
        output.reserve(max_size);

        // We need to write into the uninitialized part of the vector.
        // Safe way: resize with 0, then compress_into, then truncate?
        // Or use unsafe set_len.

        // Let's use a temporary buffer approach if we want to be 100% safe without unsafe code,
        // BUT that defeats the purpose.
        // lz4_flex::compress_into writes to a slice.

        let current_len = output.len();
        output.resize(current_len + max_size, 0);

        match lz4_flex::block::compress_into(data, &mut output[current_len..]) {
            Ok(bytes_written) => {
                output.truncate(current_len + bytes_written);
                Ok(())
            }
            Err(e) => {
                output.truncate(current_len); // Restore
                Err(ParcodeError::Compression(e.to_string()))
            }
        }
    }
}

// --- REGISTRY ---

/// Centralized registry for compression algorithms.
///
/// The registry maps algorithm IDs (stored in the file format) to
/// specific `Compressor` implementations.
#[derive(Debug)]
pub struct CompressorRegistry {
    algorithms: Vec<Option<Box<dyn Compressor>>>,
}

impl CompressorRegistry {
    /// Creates a new registry with default algorithms registered.
    ///
    /// *   ID 0: `NoCompression`
    /// *   ID 1: `Lz4Compressor` (if `lz4_flex` feature is enabled)
    pub fn new() -> Self {
        let mut reg = Self {
            algorithms: (0..8).map(|_| None).collect(),
        };

        // ID 0: NoCompression
        reg.register(Box::new(NoCompression));

        // ID 1: Lz4
        #[cfg(feature = "lz4_flex")]
        reg.register(Box::new(Lz4Compressor));

        reg
    }

    /// Registers a new compressor.
    ///
    /// The compressor's ID (returned by `algo.id()`) determines its slot in the registry.
    /// If a compressor with the same ID is already registered, it will be overwritten.
    pub fn register(&mut self, algo: Box<dyn Compressor>) {
        let id = algo.id() as usize;

        // Ensure the vector is large enough to hold the new ID.
        if id >= self.algorithms.len() {
            self.algorithms.resize_with(id + 1, || None);
        }

        // `resize_with` guarantees the index is valid.
        let slot = self
            .algorithms
            .get_mut(id)
            .expect("Registry vector resized but index not found. This is a bug.");

        *slot = Some(algo);
    }

    /// Retrieves a compressor by its ID.
    ///
    /// # Errors
    /// Returns `ParcodeError::Compression` if the ID is not registered.
    pub fn get(&self, id: u8) -> Result<&dyn Compressor> {
        let idx = usize::from(id);
        if idx < self.algorithms.len()
            && let Some(algo) = self.algorithms.get(idx).and_then(|opt| opt.as_ref())
        {
            return Ok(algo.as_ref());
        }

        Err(ParcodeError::Compression(format!(
            "Algorithm ID {} is not registered or available",
            id
        )))
    }
}

impl Default for CompressorRegistry {
    fn default() -> Self {
        Self::new()
    }
}

===== src\error.rs =====
//! Centralized error handling for Parcode.
//!
//! This module strictly avoids panics, providing a robust `Result` type
//! that propagates context up the stack.

use std::fmt;
use std::io;
use std::sync::Arc;

/// A specialized Result type for Parcode operations.
pub type Result<T> = std::result::Result<T, ParcodeError>;

/// The master error enum covering all failure domains.
#[derive(Debug, Clone)]
pub enum ParcodeError {
    /// Low-level I/O failure (disk full, permissions, etc.).
    Io(Arc<io::Error>),

    /// Serialization/Deserialization failure (bincode).
    Serialization(String),

    /// Compression algorithm failure.
    Compression(String),

    /// The file format is invalid, corrupted, or version mismatch.
    Format(String),

    /// Logic error in the graph scheduler (should not happen in prod).
    Internal(String),
}

impl fmt::Display for ParcodeError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Self::Io(e) => write!(f, "I/O Error: {e}"),
            Self::Serialization(s) => write!(f, "Serialization Error: {s}"),
            Self::Compression(s) => write!(f, "Compression Error: {s}"),
            Self::Format(s) => write!(f, "Format Error: {s}"),
            Self::Internal(s) => write!(f, "Internal Logic Error: {s}"),
        }
    }
}

impl std::error::Error for ParcodeError {
    fn source(&self) -> Option<&(dyn std::error::Error + 'static)> {
        match self {
            Self::Io(e) => Some(e),
            _ => None,
        }
    }
}

impl From<io::Error> for ParcodeError {
    fn from(err: io::Error) -> Self {
        Self::Io(Arc::new(err))
    }
}

===== src\executor.rs =====
//! The High-Performance Parallel Executor.
//!
//! This module orchestrates the "Bottom-Up" execution strategy. It is designed
//! to be purely reactive: completed children trigger the scheduling of their parents,
//! eliminating the need for a central polling loop and reducing latency.

use std::sync::Mutex;
use std::sync::atomic::{AtomicBool, Ordering};

use crate::compression::CompressorRegistry;
use crate::error::{ParcodeError, Result};
use crate::format::{ChildRef, MetaByte};
use crate::graph::{Node, TaskGraph};
use crate::io::SeqWriter;

/// Context shared among all worker threads.
struct ExecutionContext<'a, 'graph> {
    graph: &'graph TaskGraph<'a>, // The graph holds data living for 'a
    writer: &'graph SeqWriter,
    registry: &'graph CompressorRegistry,
    abort_flag: AtomicBool,
    error_capture: Mutex<Option<ParcodeError>>,
    root_result: Mutex<Option<ChildRef>>,
}

impl<'a, 'graph> ExecutionContext<'a, 'graph> {
    fn signal_error(&self, err: ParcodeError) {
        let mut guard = self.error_capture.lock().unwrap_or_else(|p| p.into_inner());
        if guard.is_none() {
            *guard = Some(err);
            self.abort_flag.store(true, Ordering::SeqCst);
        }
    }

    fn should_abort(&self) -> bool {
        self.abort_flag.load(Ordering::Relaxed)
    }

    fn capture_root_result(&self, result: ChildRef) {
        let mut guard = self.root_result.lock().unwrap_or_else(|p| p.into_inner());
        *guard = Some(result);
    }
}

/// Entry point for the execution engine.
///
/// This function consumes the graph (conceptually) and drives the I/O writing process.
/// It returns the `ChildRef` of the Root Chunk, which is needed to write the Global Header.
pub fn execute_graph<'a>(
    graph: &TaskGraph<'a>,
    writer: &SeqWriter,
    registry: &CompressorRegistry,
) -> Result<ChildRef> {
    // 1. Setup the shared context.
    let ctx = ExecutionContext {
        graph,
        writer,
        registry,
        abort_flag: AtomicBool::new(false),
        error_capture: Mutex::new(None),
        root_result: Mutex::new(None),
    };

    // 2. Identify initial leaves (Nodes with 0 dependencies).
    // These are the spark plugs that start the engine.
    let leaves: Vec<&Node<'a>> = graph
        .nodes()
        .iter()
        .filter(|n| n.atomic_deps.load(Ordering::SeqCst) == 0)
        .collect();

    if leaves.is_empty() && !graph.is_empty() {
        return Err(ParcodeError::Internal(
            "Graph has nodes but no leaves. Cyclic dependency detected.".into(),
        ));
    }

    // 3. Launch the Rayon Scope.
    // The scope ensures all spawned threads complete before this block exits.
    rayon::scope(|s| {
        let ctx_ref = &ctx;
        for leaf in leaves {
            s.spawn(move |s| process_node(s, ctx_ref, leaf));
        }
    });

    // 4. Check for errors after the scope joins.
    if ctx.should_abort() {
        let guard = ctx.error_capture.lock().unwrap_or_else(|p| p.into_inner());
        if let Some(err) = guard.as_ref() {
            return Err(err.clone());
        }
        return Err(ParcodeError::Internal("Unknown execution error".into()));
    }

    // 5. Return the Root Result.
    let root_guard = ctx
        .root_result
        .lock()
        .map_err(|_| ParcodeError::Internal("Root result mutex poisoned".into()))?;

    (*root_guard).ok_or_else(|| ParcodeError::Internal("Graph execution incomplete".into()))
}

/// The worker function executed by Rayon threads.
/// It handles Serialization -> Compression -> Writing -> Notification.
fn process_node<'scope, 'a>(
    scope: &rayon::Scope<'scope>,
    ctx: &'scope ExecutionContext<'a, 'scope>,
    node: &'scope Node<'a>,
) {
    // 0. Fast abort check
    if ctx.should_abort() {
        return;
    }

    // --- STEP 1: PREPARE DATA (CPU BOUND) ---

    let completed_children_raw = {
        let lock = node
            .completed_children
            .lock()
            .map_err(|_| ParcodeError::Internal("Node mutex poisoned".into()));
        match lock {
            Ok(mut guard) => std::mem::take(&mut *guard),
            Err(e) => {
                ctx.signal_error(e);
                return;
            }
        }
    };

    // Retrieve the results from children (if any).
    // We lock the mutex, take the vector out (swap with empty) to consume it efficiently.
    // Since we used slots, the order is already correct. We just need to unwrap the Options.
    let children_refs: Vec<ChildRef> = match completed_children_raw
        .into_iter()
        .map(|opt| opt.ok_or_else(|| ParcodeError::Internal("Missing child result".into())))
        .collect()
    {
        Ok(v) => v,
        Err(e) => {
            ctx.signal_error(e);
            return;
        }
    };

    // A node is "Chunkable" (Bit 0 set) if it had children dependencies.
    // Note: The optimizer pass (Phase 2) might have removed children and inlined them,
    // so we trust `children_refs`.
    let is_chunkable = !children_refs.is_empty();

    // Execute the job (Serialization).
    // This is where the user's `Vec<T>` turns into bytes.
    // We pass the children refs so they can be embedded in the footer/table if needed
    // by the specific implementation, or we append the standard footer below.
    let raw_payload = match node.job.execute(&children_refs) {
        Ok(bytes) => bytes,
        Err(e) => {
            ctx.signal_error(e);
            return;
        }
    };

    // --- STEP 2 & 3: COMPRESSION & FORMATTING (CPU BOUND) ---

    // 1. Get Job Config
    let config = node.job.config();

    // 2. Find Algorithm
    let compressor = match ctx.registry.get(config.compression_id) {
        Ok(c) => c,
        Err(e) => {
            ctx.signal_error(e);
            return;
        }
    };

    // 3. Prepare Final Buffer
    // We need to calculate footer size to reserve space.
    let footer_size = if is_chunkable {
        (children_refs.len() * ChildRef::SIZE) + 4
    } else {
        0
    };

    // Heuristic: Allocate enough for raw payload + footer + meta.
    // If compression shrinks it, great. If it expands (rare), it will realloc.
    let estimated_capacity = raw_payload.len() + footer_size + 1;
    let mut final_buffer = Vec::with_capacity(estimated_capacity);

    // 4. Compress directly into final buffer
    if let Err(e) = compressor.compress_append(&raw_payload, &mut final_buffer) {
        ctx.signal_error(e);
        return;
    }

    // 5. Append Footer
    if is_chunkable {
        for child in &children_refs {
            final_buffer.extend_from_slice(&child.to_bytes());
        }
        let count = u32::try_from(children_refs.len()).unwrap_or(u32::MAX);
        final_buffer.extend_from_slice(&count.to_le_bytes());
    }

    // 6. Append MetaByte
    let meta = MetaByte::new(is_chunkable, compressor.id());
    final_buffer.push(meta.as_u8());

    // --- STEP 4: WRITING (I/O BOUND) ---
    // This is the only serialization point (Mutex).

    let write_result = ctx.writer.write_all(&final_buffer);
    let offset = match write_result {
        Ok(off) => off,
        Err(e) => {
            ctx.signal_error(e);
            return;
        }
    };

    // Create the Reference for this newly written chunk.
    let my_ref = ChildRef {
        offset,
        length: final_buffer.len() as u64,
    };

    // --- STEP 5: PROPAGATION ---

    if let Some(parent_id) = node.parent {
        let parent_node = ctx.graph.get_node(parent_id);

        // A. Register our result in the parent
        let slot = node.parent_slot.expect("Parent set but slot missing");
        if let Err(e) = parent_node.register_child_result(slot, my_ref) {
            ctx.signal_error(e);
            return;
        }

        // B. Decrement parent's dependency counter.
        // `fetch_sub` returns the PREVIOUS value.
        // If previous was 1, it means it is NOW 0 -> Ready to fire.
        let prev_deps = parent_node.atomic_deps.fetch_sub(1, Ordering::SeqCst);

        if prev_deps == 1 {
            // We are the last child! We have the honor of waking the parent.
            // Spawn into the existing scope.
            scope.spawn(move |s| process_node(s, ctx, parent_node));
        }
    } else {
        // We have no parent. We are the ROOT.
        ctx.capture_root_result(my_ref);
    }
}

===== src\format.rs =====
//! Defines the physical binary layout of Parcode V4 files.
//!
//! # V4 Layout Strategy
//! The file consists of a sequence of Chunks written in a bottom-up order (children first),
//! followed by a Global Header at the very end of the file.
//!
//! File: `[Chunk 0] [Chunk 1] ... [Root Chunk] [Global Header]`
//!
//! ## Chunk Anatomy
//! Each chunk is self-contained:
//! `[ Compressed Payload ] [ Children Table (Optional) ] [ MetaByte ]`

use crate::error::{ParcodeError, Result};

/// Magic bytes identifying the file format: "PAR4".
pub const MAGIC_BYTES: [u8; 4] = *b"PAR4";

/// The fixed size of the Global Header.
/// Magic(4) + Version(2) + RootOffset(8) + RootLength(8) + Checksum(4) = 26
pub const GLOBAL_HEADER_SIZE: usize = 26;

/// Configuration flags for a specific chunk, stored in the last byte.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct MetaByte(u8);

impl MetaByte {
    const CHUNKABLE_MASK: u8 = 0b0000_0001; // Bit 0
    const COMPRESSION_MASK: u8 = 0b0000_1110; // Bits 1-3

    /// Creates a new `MetaByte`.
    pub fn new(is_chunkable: bool, compression_id: u8) -> Self {
        let mut byte = 0;
        if is_chunkable {
            byte |= Self::CHUNKABLE_MASK;
        }
        // Compress ID lives in bits 1-3
        byte |= (compression_id & 0x07) << 1;
        Self(byte)
    }

    /// decodes the byte.
    pub fn from_byte(byte: u8) -> Self {
        Self(byte)
    }

    /// Returns true if the chunk contains references to children.
    pub fn is_chunkable(&self) -> bool {
        (self.0 & Self::CHUNKABLE_MASK) != 0
    }

    /// Returns the compression algorithm ID (0-7).
    pub fn compression_method(&self) -> u8 {
        (self.0 & Self::COMPRESSION_MASK) >> 1
    }

    /// Returns the raw byte representation.
    pub fn as_u8(&self) -> u8 {
        self.0
    }
}

/// Represents a reference to a child chunk stored within a parent chunk.
/// This allows the reader to locate dependencies without deserializing the payload.
#[derive(Debug, Clone, Copy)]
pub struct ChildRef {
    /// Absolute offset in the file where the child chunk starts.
    pub offset: u64,
    /// Total length of the child chunk (including meta-byte).
    pub length: u64,
}

impl ChildRef {
    /// The size in bytes of a serialized `ChildRef`.
    pub const SIZE: usize = 16; // 8 bytes offset + 8 bytes length

    /// Serializes to a fixed-size byte array (Little Endian).
    pub fn to_bytes(&self) -> [u8; Self::SIZE] {
        let mut buf = [0u8; Self::SIZE];
        buf[0..8].copy_from_slice(&self.offset.to_le_bytes());
        buf[8..16].copy_from_slice(&self.length.to_le_bytes());
        buf
    }

    /// Deserializes from a fixed-size byte array.
    pub fn from_bytes(bytes: &[u8]) -> Result<Self> {
        if bytes.len() < Self::SIZE {
            return Err(ParcodeError::Format("Buffer too small for ChildRef".into()));
        }
        let offset = u64::from_le_bytes(
            bytes
                .get(0..8)
                .and_then(|s| s.try_into().ok())
                .unwrap_or([0; 8]),
        );
        let length = u64::from_le_bytes(
            bytes
                .get(8..16)
                .and_then(|s| s.try_into().ok())
                .unwrap_or([0; 8]),
        );
        Ok(Self { offset, length })
    }
}

/// The Global Header located at the very end of the file (Tail).
/// It points to the Root Chunk, which is the entry point for the graph.
#[derive(Debug, Clone, Copy)]
pub struct GlobalHeader {
    /// The magic bytes identifying the file format.
    pub magic: [u8; 4],
    /// The version of the file format (currently 4).
    pub version: u16,
    /// Pointer to the final Root Chunk.
    pub root_offset: u64,
    /// The total length of the Root Chunk.
    pub root_length: u64,
    /// Reserved for CRC/Checksum of the header itself.
    pub checksum: u32,
}

impl GlobalHeader {
    /// Creates a new `GlobalHeader`.
    pub fn new(root_offset: u64, root_length: u64) -> Self {
        Self {
            magic: MAGIC_BYTES,
            version: 4,
            root_offset,
            root_length,
            checksum: 0,
        }
    }

    /// Serializes the header to bytes.
    pub fn to_bytes(&self) -> [u8; GLOBAL_HEADER_SIZE] {
        let mut buf = [0u8; GLOBAL_HEADER_SIZE];
        buf[0..4].copy_from_slice(&self.magic);
        buf[4..6].copy_from_slice(&self.version.to_le_bytes());
        buf[6..14].copy_from_slice(&self.root_offset.to_le_bytes());
        buf[14..22].copy_from_slice(&self.root_length.to_le_bytes());
        buf[22..26].copy_from_slice(&self.checksum.to_le_bytes());
        buf
    }
}

===== src\graph\core.rs =====
use super::id::ChunkId;
use super::job::SerializationJob;
use crate::format::ChildRef;
use crate::{ParcodeError, Result};
use std::sync::Mutex;
use std::sync::atomic::{AtomicUsize, Ordering};

/// A single node in the dependency graph.
///
/// It holds a `SerializationJob` which may contain references to user data (`'a`).
#[derive(Debug)]
pub struct Node<'a> {
    /// The unique identifier for this node.
    pub id: ChunkId,
    /// The ID of the parent node (if any).
    /// Currently, the graph is a tree, so a node has at most one parent.
    pub parent: Option<ChunkId>,
    /// The number of dependencies (children) that must complete before this node can run.
    pub atomic_deps: AtomicUsize,

    /// The job to execute. Note the `+ 'a` bound.
    pub job: Box<dyn SerializationJob<'a> + 'a>,

    /// The slot index in the parent's `completed_children` vector.
    /// This allows the child to write its result directly to the correct position.
    pub parent_slot: Option<usize>,

    /// A list of results from completed children.
    /// This is populated as children finish execution.
    /// We use `Option` to allow random-access insertion without resizing issues,
    /// though we push in order during linking.
    pub completed_children: Mutex<Vec<Option<ChildRef>>>,
}

impl<'a> Node<'a> {
    /// Creates a new Node.
    pub fn new(id: ChunkId, job: Box<dyn SerializationJob<'a> + 'a>) -> Self {
        Self {
            id,
            parent: None,
            parent_slot: None,
            atomic_deps: AtomicUsize::new(0),
            job,
            completed_children: Mutex::new(Vec::new()),
        }
    }

    /// Registers a completed child's result.
    ///
    /// This is called by the executor when a child node finishes.
    pub fn register_child_result(&self, slot: usize, child_ref: ChildRef) -> Result<()> {
        let mut lock = self
            .completed_children
            .lock()
            .map_err(|_| ParcodeError::Internal(format!("Mutex poisoned on node {:?}", self.id)))?;

        let entry = lock.get_mut(slot).ok_or_else(|| {
            ParcodeError::Internal(format!(
                "Slot {} out of bounds for node {:?}",
                slot, self.id
            ))
        })?;
        *entry = Some(child_ref);
        Ok(())
    }
}

/// The container for the entire dependency graph.
///
/// Acts as an Arena allocator for Nodes. The graph lifetime `'a` is tied
/// to the input data provided by the user in `Parcode::save`.
#[derive(Debug)]
pub struct TaskGraph<'a> {
    nodes: Vec<Node<'a>>,
}

impl<'a> TaskGraph<'a> {
    /// Creates a new, empty `TaskGraph`.
    pub fn new() -> Self {
        Self { nodes: Vec::new() }
    }

    /// Adds a new node to the graph.
    ///
    /// Returns the `ChunkId` of the newly created node.
    pub fn add_node(&mut self, job: Box<dyn SerializationJob<'a> + 'a>) -> ChunkId {
        let id = ChunkId::new(u32::try_from(self.nodes.len()).unwrap_or(u32::MAX));
        let node = Node::new(id, job);
        self.nodes.push(node);
        id
    }

    /// Links a parent node to a child node.
    ///
    /// This increments the parent's dependency count and sets the child's parent pointer.
    pub fn link_parent_child(&mut self, parent_id: ChunkId, child_id: ChunkId) {
        let parent_node_idx = parent_id.as_u32() as usize;
        let child_node_idx = child_id.as_u32() as usize;
        let parent_node = self
            .nodes
            .get(parent_node_idx)
            .expect("Parent node not found");
        parent_node.atomic_deps.fetch_add(1, Ordering::SeqCst);

        // Reserve slot in parent
        let slot = {
            let mut guard = parent_node
                .completed_children
                .lock()
                .expect("Mutex poisoned");
            let idx = guard.len();
            guard.push(None);
            idx
        };

        let child_node = self
            .nodes
            .get_mut(child_node_idx)
            .expect("Child node not found");
        child_node.parent = Some(parent_id);
        child_node.parent_slot = Some(slot);
    }

    /// Retrieves a reference to a node by its ID.
    pub fn get_node(&self, id: ChunkId) -> &Node<'a> {
        self.nodes
            .get(id.as_u32() as usize)
            .expect("Node ID out of bounds")
    }

    /// Returns true if the graph has no nodes.
    pub fn is_empty(&self) -> bool {
        self.nodes.is_empty()
    }

    /// Returns the number of nodes in the graph.
    pub fn len(&self) -> usize {
        self.nodes.len()
    }

    /// Returns a slice containing all nodes in the graph.
    pub fn nodes(&self) -> &[Node<'a>] {
        &self.nodes
    }
}

// Manual Default impl to avoid restrictive bounds on 'a
impl<'a> Default for TaskGraph<'a> {
    fn default() -> Self {
        Self::new()
    }
}

===== src\graph\id.rs =====
use std::fmt;

/// A strong type representing a unique identifier for a node in the task graph.
/// This maps 1:1 to a future chunk on disk.
#[derive(Clone, Copy, PartialEq, Eq, Hash, PartialOrd, Ord)]
pub struct ChunkId(u32); // u32 is sufficient for 4 billion chunks per file.

impl ChunkId {
    /// Creates a new `ChunkId`.
    /// Restrict visibility to the graph module to prevent arbitrary creation.
    pub(crate) fn new(id: u32) -> Self {
        Self(id)
    }

    /// Returns the raw numeric value.
    pub fn as_u32(&self) -> u32 {
        self.0
    }
}

impl fmt::Debug for ChunkId {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "ChunkId({})", self.0)
    }
}

impl fmt::Display for ChunkId {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "#{}", self.0)
    }
}

===== src\graph\job.rs =====
use crate::error::Result;
use crate::format::ChildRef;

/// Execution configuration for a specific node.
/// Kept small (1 byte) for efficient pass-by-copy.
#[derive(Default, Debug, Clone, Copy, PartialEq, Eq)]
pub struct JobConfig {
    /// Compression algorithm ID.
    /// 0 = No Compression (Default)
    /// 1 = Lz4 (if feature enabled)
    /// 2..255 = Reserved
    pub compression_id: u8,
    /// Whether to use the optimized Map Sharding strategy.
    pub is_map: bool,
}

//impl Default for JobConfig {
//    fn default() -> Self {
//        Self { compression_id: 0 }
//    }
//}

/// Represents a unit of work: a piece of data that knows how to serialize itself.
///
/// # Lifetimes
/// * `'a`: The lifetime of the data being serialized. This allows the Job to hold
///   references (e.g., `&'a [u8]`) instead of owning the data, enabling Zero-Copy writes.
pub trait SerializationJob<'a>: Send + Sync {
    /// Executes the serialization logic, producing a raw byte buffer.
    fn execute(&self, children_refs: &[ChildRef]) -> Result<Vec<u8>>;

    /// Returns an estimated size in bytes for scheduling heuristics.
    fn estimated_size(&self) -> usize;

    /// Returns the specific configuration for this job.
    fn config(&self) -> JobConfig {
        JobConfig::default()
    }
}

impl<'a> std::fmt::Debug for Box<dyn SerializationJob<'a> + 'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "SerializationJob(size={}, algo={})",
            self.estimated_size(),
            self.config().compression_id
        )
    }
}

===== src\graph\mod.rs =====
//! Core graph definitions for the Parcode execution engine.
//!
//! This module defines the `TaskGraph`, `Node`, and `SerializationJob` structures
//! that form the backbone of the parallel serialization process.

/// Defines the `TaskGraph` and `Node` structures.
pub mod core;
/// Defines the `ChunkId` type.
pub mod id;
/// Defines the `SerializationJob` trait and `JobConfig`.
pub mod job;

pub use core::{Node, TaskGraph};
pub use id::ChunkId;
pub use job::{JobConfig, SerializationJob};

===== src\io.rs =====
//! High-Performance Sequential Writer.
//!
//! # Architecture: Aggressive Buffering
//! Instead of complex channel passing (which introduces scheduling jitter),
//! we use a `Mutex` protecting a massive `BufWriter` (16MB).
//!
//! - **Latency:** Writes are effectively `memcpy` operations into RAM until the buffer fills.
//! - **Throughput:** Flushes happen in huge chunks, saturating SSD bandwidth.
//! - **Stability:** Eliminates channel backpressure "stop-and-go" behavior.

use crate::error::{ParcodeError, Result};
use std::fs::File;
use std::io::{BufWriter, Write};
use std::path::Path;
use std::sync::Mutex;

/// We use a 16MB buffer.
/// This allows ~128 chunks of 128KB to be written purely in memory
/// before triggering a syscall.
const WRITE_BUFFER_SIZE: usize = 16 * 1024 * 1024;

/// A thread-safe, buffered sequential writer.
///
/// This writer is designed to be shared across multiple threads (via `Mutex`)
/// to allow concurrent graph execution to serialize data, while ensuring
/// that the actual disk writes happen sequentially and efficiently.
#[derive(Debug)]
pub struct SeqWriter {
    inner: Mutex<WriterState>,
}

#[derive(Debug)]
struct WriterState {
    writer: BufWriter<File>,
    current_offset: u64,
}

impl SeqWriter {
    /// Opens the file with an optimized buffer configuration.
    ///
    /// The file is created (truncated if it exists) and wrapped in a large `BufWriter`.
    pub fn create(path: &Path) -> Result<Self> {
        let file = File::create(path)?;
        Ok(Self {
            inner: Mutex::new(WriterState {
                writer: BufWriter::with_capacity(WRITE_BUFFER_SIZE, file),
                current_offset: 0,
            }),
        })
    }

    /// Writes a chunk of data atomically to the file sequence.
    ///
    /// Returns the byte offset where the chunk begins.
    pub fn write_all(&self, buffer: &[u8]) -> Result<u64> {
        // Acquire lock.
        // Since we mostly do memcpy, contention is extremely low.
        let mut state = self
            .inner
            .lock()
            .map_err(|_| ParcodeError::Internal("Writer mutex poisoned".into()))?;

        let start_offset = state.current_offset;

        // This call usually just copies memory.
        // It only blocks for disk I/O once every ~16MB of data.
        state.writer.write_all(buffer)?;

        state.current_offset += buffer.len() as u64;

        Ok(start_offset)
    }

    /// Forces data to disk.
    pub fn flush(&self) -> Result<()> {
        let mut state = self
            .inner
            .lock()
            .map_err(|_| ParcodeError::Internal("Writer mutex poisoned".into()))?;
        state.writer.flush()?;
        Ok(())
    }
}

===== src\lib.rs =====
//! # Parcode V3
//!
//! A high-performance, graph-based serialization library for Rust.
//!
//! Parcode is designed to handle large, complex data structures by breaking them down into
//! a dependency graph of chunks. This allows for:
//!
//! *   **Parallel Serialization:** Independent chunks are serialized and compressed concurrently.
//! *   **Parallel Deserialization:** Reading utilizes memory mapping and parallel reconstruction.
//! *   **Zero-Copy Operations:** Where possible, data is read directly from the memory-mapped file.
//! *   **Streaming/Partial Reads:** Large collections can be iterated over without loading the entire dataset into RAM.
//!
//! ## Core Concepts
//!
//! *   **`TaskGraph`:** The central structure representing the object graph to be serialized.
//! *   **`Executor`:** The engine that drives the parallel execution of the graph (serialization -> compression -> I/O).
//! *   **`Reader`:** The component responsible for mapping the file and reconstructing objects.
//! *   **`Visitor`:** The trait that allows types to define how they should be split into graph nodes.

#![deny(unsafe_code)]
#![deny(clippy::unwrap_used)]
#![deny(clippy::panic)]
#![warn(missing_docs)]

pub mod api;
pub mod compression;
pub mod error;
pub mod executor;
pub mod format;
pub mod graph;
pub mod io;
/// Map optimization strategies.
pub mod map;
pub mod reader;
pub mod visitor;

mod visitor_impls;

// --- MACRO SUPPORT MODULES ---

/// Runtime utilities used by the derived code.
#[doc(hidden)]
pub mod rt;

/// Internal re-exports for the macro to ensure dependencies are available.
#[doc(hidden)]
pub mod internal {
    pub use bincode;
    pub use serde;
}

// --- RE-EXPORTS ---

#[cfg(feature = "lz4_flex")]
pub use compression::Lz4Compressor;
pub use compression::{Compressor, NoCompression};

pub use api::Parcode;
pub use error::{ParcodeError, Result};
pub use reader::ParcodeReader;

// Re-export the derive macro so it is accessible as `parcode::ParcodeObject`
pub use parcode_derive::ParcodeObject;

/// Constants used throughout the library.
pub mod constants {
    /// The default buffer size for I/O operations.
    pub const DEFAULT_BUFFER_SIZE: usize = 8 * 1024;
}

===== src\map.rs =====
// src/map.rs

//! PLACEHOLDER

use crate::error::{ParcodeError, Result};
use crate::format::ChildRef;
use crate::graph::SerializationJob;
use serde::Serialize;
use std::hash::{Hash, Hasher};
use twox_hash::XxHash64;

pub(crate) fn hash_key<K: Hash>(key: &K) -> u64 {
    let mut hasher = XxHash64::with_seed(0);
    key.hash(&mut hasher);
    hasher.finish()
}

/// Job for a single Hash Shard.
/// Layout: [Count(4)] [Padding(4)] [Hashes(8*N)] [Offsets(4*N)] [DataBlob]
#[derive(Debug)]
pub struct MapShardJob<'a, K, V> {
    /// Items to be serialized in this shard.
    pub items: Vec<(&'a K, &'a V)>,
}

impl<'a, K, V> SerializationJob<'a> for MapShardJob<'a, K, V>
where
    K: Serialize + Hash + Sync,
    V: Serialize + Sync,
{
    fn execute(&self, _: &[ChildRef]) -> Result<Vec<u8>> {
        let count = self.items.len();
        if count == 0 {
            return Ok(Vec::new());
        }

        let mut data_blob = Vec::new();
        let mut offsets = Vec::with_capacity(count * 4);
        let mut hashes = Vec::with_capacity(count * 8);

        let mut cursor = std::io::Cursor::new(&mut data_blob);

        for (k, v) in &self.items {
            // 1. Hash
            hashes.extend_from_slice(&hash_key(k).to_le_bytes());

            // 2. Offset (relative to data start)
            let pos = cursor.position() as u32;
            offsets.extend_from_slice(&pos.to_le_bytes());

            // 3. Data (Key + Value for collision check)
            bincode::serde::encode_into_std_write(
                &(k, v),
                &mut cursor,
                bincode::config::standard(),
            )
            .map_err(|e| ParcodeError::Serialization(e.to_string()))?;
        }

        // 4. Assembly with Alignment
        // Header: Count (4 bytes).
        // Alignment target for Hashes is 8 bytes.
        // Current size: 4. Padding needed: 4.

        let hashes_size = count * 8;
        let offsets_size = count * 4;
        let total_size = 8 + hashes_size + offsets_size + data_blob.len();

        let mut final_buf = Vec::with_capacity(total_size);

        // Write Count
        final_buf.extend_from_slice(&(count as u32).to_le_bytes());

        // Write Padding (4 bytes of zeros)
        final_buf.extend_from_slice(&[0u8; 4]);

        // Write Hashes (Aligned at offset 8)
        final_buf.extend_from_slice(&hashes);

        // Write Offsets
        final_buf.extend_from_slice(&offsets);

        // Write Data
        final_buf.extend_from_slice(&data_blob);

        Ok(final_buf)
    }

    fn estimated_size(&self) -> usize {
        // Crude estimation
        self.items.len() * (std::mem::size_of::<K>() + std::mem::size_of::<V>() + 12)
    }
}

===== src\reader.rs =====
//! The Read-Side Engine: Parallel Reconstruction & Random Access.
//!
//! This module implements the logic to map a `parcode` file into memory and reconstruct
//! complex data structures efficiently.
//!
//! # Core Architectures
//!
//! 1. **Lazy Traversal:** The file is Memory Mapped (`mmap`). We only read/decompress bytes
//!    when a specific node is requested.
//!
//! 2. **O(1) Arithmetic Navigation:** Using the RLE metadata stored in container nodes,
//!    we can calculate exactly which physical Chunk holds the Nth item of a collection,
//!    allowing random access without linear scans.
//!
//! 3. **Parallel Zero-Copy Stitching:** When reconstructing large `Vec<T>`, we:
//!    - Pre-allocate the final uninitialized memory buffer.
//!    - Calculate the destination offset for every shard.
//!    - Use `rayon` to dispatch parallel workers.
//!    - Each worker decodes a shard and writes directly into the final buffer.
//!    - **Result:** Maximized memory bandwidth, zero intermediate allocations.

use memmap2::Mmap;
use rayon::prelude::*;
use serde::{Deserialize, de::DeserializeOwned};
use std::borrow::Cow;
use std::collections::HashMap;
use std::fs::File;
use std::hash::Hash;
use std::marker::PhantomData;
use std::mem::{ManuallyDrop, MaybeUninit};
use std::path::Path;
use std::sync::Arc;

use crate::compression::CompressorRegistry;
use crate::error::{ParcodeError, Result};
use crate::format::{ChildRef, GLOBAL_HEADER_SIZE, GlobalHeader, MAGIC_BYTES, MetaByte};
use crate::rt::ParcodeLazyRef;

// --- TRAIT SYSTEM FOR AUTOMATIC STRATEGY SELECTION ---

/// A trait for types that know how to reconstruct themselves from a `ChunkNode`.
///
/// This allows the high-level API (`Parcode::read`) to automatically select the
/// optimal strategy (Parallel vs Sequential) based on the type being read.
pub trait ParcodeNative: Sized {
    /// Reconstructs the object from the given graph node.
    fn from_node(node: &ChunkNode<'_>) -> Result<Self>;
}

/// Optimized implementation for Vectors: Uses Parallel Stitching.
impl<T> ParcodeNative for Vec<T>
where
    T: DeserializeOwned + Send + Sync,
{
    fn from_node(node: &ChunkNode<'_>) -> Result<Self> {
        node.decode_parallel_collection()
    }
}

impl<K, V> ParcodeNative for HashMap<K, V>
where
    K: DeserializeOwned + Eq + Hash + Send + Sync,
    V: DeserializeOwned + Send + Sync,
{
    fn from_node(node: &ChunkNode<'_>) -> Result<Self> {
        // Usar la lógica de ParcodeMapPromise::load() o similar
        // Dado que ParcodeMapPromise está en 'rt', y 'rt' usa 'reader',
        // podemos implementar la lógica aquí o usar rt si no hay ciclo.
        // Mejor implementar la lógica de "Load All Shards" aquí.

        // 1. Leer contenedor (num shards)
        let container_payload = node.read_raw()?;
        if container_payload.len() < 4 {
            return Ok(HashMap::new());
        }

        // Si el payload es grande, quizás no es un mapa sharded, sino un blob bincode directo.
        // ParcodeVisitor para Map tiene dos modos: Optimizado (Sharded) y Estándar (Blob).
        // Si es Blob, node.child_count == 0.

        if node.child_count == 0 {
            return node.decode(); // Fallback a Bincode normal
        }

        // Si tiene hijos, es Sharded Map.
        // Reutilizamos la lógica de iterar shards.
        let shards = node.children()?;
        let mut map = HashMap::new();

        for shard in shards {
            let payload = shard.read_raw()?;
            if payload.len() < 8 {
                continue;
            }

            let count = u32::from_le_bytes(payload[0..4].try_into().unwrap()) as usize;
            let offsets_start = 8 + (count * 8);
            let data_start = offsets_start + (count * 4);
            let offsets_bytes = &payload[offsets_start..data_start];

            for i in 0..count {
                let off_bytes = &offsets_bytes[i * 4..(i + 1) * 4];
                let offset = u32::from_le_bytes(off_bytes.try_into().unwrap()) as usize;
                let data_slice = &payload[data_start + offset..];
                let (k, v) =
                    bincode::serde::decode_from_slice(data_slice, bincode::config::standard())
                        .map_err(|e| ParcodeError::Serialization(e.to_string()))?
                        .0;
                map.insert(k, v);
            }
        }
        Ok(map)
    }
}

// --- CORE READER HANDLE ---

/// The main handle for an open Parcode file.
///
/// It holds the memory map (thread-safe via Arc), the global file header,
/// and the registry of available decompression algorithms.
/// Cloning this struct is cheap (increments Arc ref count).
#[derive(Debug)]
pub struct ParcodeReader {
    /// Memory-mapped file content.
    mmap: Arc<Mmap>,
    /// Parsed global footer/header information.
    header: GlobalHeader,
    /// Total size of the file in bytes.
    file_size: u64,
    /// Registry containing available decompression algorithms (Lz4, etc.).
    registry: CompressorRegistry,
}

impl ParcodeReader {
    /// Opens a Parcode file, maps it into memory, and validates integrity.
    ///
    /// # Errors
    /// Returns error if the file does not exist, is smaller than the header,
    /// or contains invalid magic bytes/version.
    pub fn open<P: AsRef<Path>>(path: P) -> Result<Self> {
        let file = File::open(path)?;
        let file_size = file.metadata()?.len();

        if file_size < GLOBAL_HEADER_SIZE as u64 {
            return Err(ParcodeError::Format(
                "File is smaller than the global header".into(),
            ));
        }

        // SAFETY: Mmap is fundamentally unsafe in the presence of external modification
        // (e.g., another process truncating the file). We assume the file is treated
        // as immutable by the OS while we read it.
        #[allow(unsafe_code)]
        let mmap = unsafe { Mmap::map(&file)? };

        // Read Global Header (Located at the very end of the file)
        let header_start = usize::try_from(file_size)
            .map_err(|_| ParcodeError::Format("File too large for address space".into()))?
            - GLOBAL_HEADER_SIZE;
        let header_bytes = mmap
            .get(header_start..)
            .ok_or_else(|| ParcodeError::Format("Header start out of bounds".into()))?;

        if header_bytes.get(0..4) != Some(&MAGIC_BYTES) {
            return Err(ParcodeError::Format(
                "Invalid Magic Bytes. Not a Parcode file.".into(),
            ));
        }

        let version = u16::from_le_bytes(
            header_bytes
                .get(4..6)
                .ok_or_else(|| ParcodeError::Format("Version out of bounds".into()))?
                .try_into()
                .map_err(|_| ParcodeError::Format("Failed to read version".into()))?,
        );
        if version != 4 {
            return Err(ParcodeError::Format(format!(
                "Unsupported version: {version}. Expected V4."
            )));
        }

        let root_offset = u64::from_le_bytes(
            header_bytes
                .get(6..14)
                .ok_or_else(|| ParcodeError::Format("Root offset out of bounds".into()))?
                .try_into()
                .map_err(|_| ParcodeError::Format("Failed to read root_offset".into()))?,
        );
        let root_length = u64::from_le_bytes(
            header_bytes
                .get(14..22)
                .ok_or_else(|| ParcodeError::Format("Root length out of bounds".into()))?
                .try_into()
                .map_err(|_| ParcodeError::Format("Failed to read root_length".into()))?,
        );
        let checksum = u32::from_le_bytes(
            header_bytes
                .get(22..26)
                .ok_or_else(|| ParcodeError::Format("Checksum out of bounds".into()))?
                .try_into()
                .map_err(|_| ParcodeError::Format("Failed to read checksum".into()))?,
        );

        Ok(Self {
            mmap: Arc::new(mmap),
            header: GlobalHeader {
                magic: MAGIC_BYTES,
                version,
                root_offset,
                root_length,
                checksum,
            },
            file_size,
            // Initialize registry with default algorithms (NoCompression, Lz4 if enabled)
            registry: CompressorRegistry::new(),
        })
    }

    /// Helper to read a u32 from a byte slice (Little Endian).
    fn read_u32(slice: &[u8]) -> Result<u32> {
        slice
            .try_into()
            .map(u32::from_le_bytes)
            .map_err(|_| ParcodeError::Format("Failed to read u32".into()))
    }

    /// Returns a cursor to the Root Chunk of the object graph.
    pub fn root(&self) -> Result<ChunkNode<'_>> {
        self.get_chunk(self.header.root_offset, self.header.root_length)
    }

    /// Internal: Resolves a physical offset/length into a `ChunkNode`.
    /// Parses the footer to determine if the chunk has children.
    ///
    /// # Arguments
    /// * `offset`: Absolute byte offset in the file.
    /// * `length`: Total length of the chunk including metadata.
    fn get_chunk(&self, offset: u64, length: u64) -> Result<ChunkNode<'_>> {
        if offset + length > self.file_size {
            return Err(ParcodeError::Format(format!(
                "Chunk out of bounds: {} + {}",
                offset, length
            )));
        }
        let chunk_end = usize::try_from(offset + length)
            .map_err(|_| ParcodeError::Format("Chunk end exceeds address space".into()))?;

        // Read the MetaByte (Last byte of the chunk)
        let meta = MetaByte::from_byte(
            *self
                .mmap
                .get(chunk_end - 1)
                .ok_or_else(|| ParcodeError::Format("MetaByte out of bounds".into()))?,
        );

        let mut child_count = 0;
        let mut payload_end = chunk_end - 1; // Default: payload ends just before MetaByte

        if meta.is_chunkable() {
            // Layout: [Payload] ... [ChildRefs] [ChildCount (4 bytes)] [MetaByte (1 byte)]
            if length < 5 {
                return Err(ParcodeError::Format("Chunk too small for metadata".into()));
            }

            let count_start = chunk_end - 5;
            let child_count_bytes = self
                .mmap
                .get(count_start..count_start + 4)
                .ok_or_else(|| ParcodeError::Format("Child count out of bounds".into()))?;
            child_count = Self::read_u32(child_count_bytes)?;

            let footer_size = child_count as usize * ChildRef::SIZE;
            let total_meta_size = 1 + 4 + footer_size;

            if length < total_meta_size as u64 {
                return Err(ParcodeError::Format("Invalid footer size".into()));
            }
            payload_end = chunk_end - total_meta_size;
        }

        Ok(ChunkNode {
            reader: self,
            offset,
            length,
            meta,
            child_count,
            payload_end_offset: offset + (payload_end as u64 - offset),
        })
    }

    /// Reads an object lazily, returning a generated Mirror struct.
    /// This parses local fields immediately but keeps remote fields as handles.
    ///
    /// The returned Lazy object is tied to the lifetime of the `ParcodeReader`.
    pub fn read_lazy<'a, T>(&'a self) -> Result<T::Lazy>
    where
        T: ParcodeLazyRef<'a>,
    {
        let root = self.root()?;
        T::create_lazy(root)
    }
}

// --- CHUNK NODE API ---

/// A lightweight cursor pointing to a specific node in the dependency graph.
///
/// This struct contains the logic to read, decompress, and navigate from this node.
/// It is a "view" into the `ParcodeReader` and holds a lifetime reference to it.
#[derive(Debug, Clone)]
pub struct ChunkNode<'a> {
    reader: &'a ParcodeReader,
    /// Physical start offset.
    offset: u64,
    /// Total physical length.
    #[allow(dead_code)]
    length: u64,
    /// Parsed metadata flags.
    meta: MetaByte,
    /// Number of direct children (Shards).
    child_count: u32,
    /// Calculated end of the payload data.
    payload_end_offset: u64,
}

/// Helper struct for deserializing RLE metadata stored in vector headers.
#[derive(Deserialize, Debug, Clone)]
struct ShardRun {
    item_count: u32,
    repeat: u32,
}

impl<'a> ChunkNode<'a> {
    /// Reads and decompresses the local payload of this chunk.
    ///
    /// This returns `Cow`, so if no compression was used, it returns a direct reference
    /// to the mmap (Zero-Copy). If compressed, it allocates the decompressed buffer.
    pub fn read_raw(&self) -> Result<Cow<'a, [u8]>> {
        let start = usize::try_from(self.offset)
            .map_err(|_| ParcodeError::Format("Offset exceeds address space".into()))?;
        let end = usize::try_from(self.payload_end_offset)
            .map_err(|_| ParcodeError::Format("End offset exceeds address space".into()))?;

        if end > self.reader.mmap.len() {
            return Err(ParcodeError::Format(
                "Payload offset out of mmap bounds".into(),
            ));
        }

        let raw = self
            .reader
            .mmap
            .get(start..end)
            .ok_or_else(|| ParcodeError::Format("Payload out of bounds".into()))?;
        let method_id = self.meta.compression_method();

        // Delegate decompression to the registry.
        // This supports pluggable algorithms (e.g., Lz4).
        self.reader.registry.get(method_id)?.decompress(raw)
    }

    /// Returns a list of all direct child nodes.
    ///
    /// This allows manual traversal of the dependency graph (e.g., iterating over specific shards).
    /// Note: This does not deserialize the children, only loads their metadata (offsets).
    pub fn children(&self) -> Result<Vec<Self>> {
        let mut list = Vec::with_capacity(self.child_count as usize);
        for i in 0..self.child_count {
            list.push(self.get_child_by_index(i as usize)?);
        }
        Ok(list)
    }

    /// Standard single-threaded deserialization.
    /// Use this for leaf nodes or simple structs that fit in memory.
    pub fn decode<T: DeserializeOwned>(&self) -> Result<T> {
        let payload = self.read_raw()?;
        bincode::serde::decode_from_slice(&payload, bincode::config::standard())
            .map(|(obj, _)| obj)
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }

    /// **Parallel Shard Reconstruction**
    ///
    /// This method reconstructs a `Vec<T>` by deserializing all shards in parallel
    /// and writing them directly into a preallocated buffer. It is designed for
    /// high‑performance scenarios where collections are split into shards.
    ///
    /// # Safety & Performance Considerations
    /// - **Uninitialized Allocation:** Uses `MaybeUninit` to allocate the final buffer
    ///   without zero‑initialization cost.
    /// - **Parallel Filling:** Uses `rayon` to concurrently populate disjoint regions.
    /// - **Ownership Management:** Wraps temporary vectors in `ManuallyDrop` to prevent
    ///   double‑free errors when moving memory via `ptr::copy`.
    /// - **Pointer Arithmetic:** Converts the buffer base pointer to `usize` to safely
    ///   share it across thread boundaries (`Send + Sync + Copy`).
    pub fn decode_parallel_collection<T>(&self) -> Result<Vec<T>>
    where
        T: DeserializeOwned + Send + Sync,
    {
        let payload = self.read_raw()?;

        // Fallback path for small vectors or leaves:
        if payload.len() < 8 {
            return self.decode::<Vec<T>>();
        }

        // 1. Parse metadata from header
        let total_items = usize::try_from(u64::from_le_bytes(
            payload
                .get(0..8)
                .ok_or_else(|| ParcodeError::Format("Payload too short for header".into()))?
                .try_into()
                .map_err(|_| ParcodeError::Format("Failed to read total_items".into()))?,
        ))
        .map_err(|_| ParcodeError::Format("total_items exceeds usize range".into()))?;
        let runs_data = payload.get(8..).unwrap_or(&[]);
        let shard_runs: Vec<ShardRun> =
            bincode::serde::decode_from_slice(runs_data, bincode::config::standard())
                .map(|(obj, _)| obj)
                .map_err(|e| ParcodeError::Serialization(e.to_string()))?;

        // 2. Expand RLE into explicit shard jobs
        let mut shard_jobs = Vec::with_capacity(self.child_count as usize);
        let mut current_shard_idx = 0;
        let mut current_global_idx: usize = 0;

        for run in shard_runs {
            let items_per_shard = run.item_count as usize;
            for _ in 0..run.repeat {
                if current_global_idx.checked_add(items_per_shard).is_none() {
                    return Err(ParcodeError::Format(
                        "Integer overflow in RLE calculation".into(),
                    ));
                }
                shard_jobs.push((current_shard_idx, current_global_idx));
                current_shard_idx += 1;
                current_global_idx += items_per_shard;
            }
        }

        if current_global_idx != total_items {
            return Err(ParcodeError::Format(format!(
                "Metadata mismatch: Header says {} items, RLE implies {}",
                total_items, current_global_idx
            )));
        }

        if shard_jobs.is_empty() {
            return Ok(Vec::new());
        }

        // 3. Allocate uninitialized buffer
        let mut result_buffer: Vec<MaybeUninit<T>> = Vec::with_capacity(total_items);

        // SAFETY: We are creating a "hole" in memory that we PROMISE to fill.
        #[allow(unsafe_code)]
        unsafe {
            result_buffer.set_len(total_items);
        }

        // 4. Perform parallel stitching
        // Trick: convert base pointer to `usize` so it can cross thread boundaries.
        let buffer_base = result_buffer.as_mut_ptr() as usize;

        shard_jobs
            .into_par_iter()
            .try_for_each(move |(shard_idx, start_idx)| -> Result<()> {
                let shard_node = self.get_child_by_index(shard_idx)?;
                // Deserialize shard into a thread-local vector
                let items: Vec<T> = shard_node.decode()?;
                let count = items.len();

                if start_idx + count > total_items {
                    return Err(ParcodeError::Format(
                        "Shard items overflowed allocated buffer".into(),
                    ));
                }

                // Prevent double‑free: wrap items so we can take ownership of bits
                let src_items = ManuallyDrop::new(items);

                #[allow(unsafe_code)]
                unsafe {
                    // Reconstruct pointer. `ptr::add` works on T units.
                    let dest_ptr = (buffer_base as *mut T).add(start_idx);
                    let src_ptr = src_items.as_ptr();

                    // Efficient memory copy
                    std::ptr::copy_nonoverlapping(src_ptr, dest_ptr, count);
                }
                Ok(())
            })?;

        // 5. Bless the buffer
        #[allow(unsafe_code)]
        let final_vec = unsafe {
            let mut manual_buffer = ManuallyDrop::new(result_buffer);
            Vec::from_raw_parts(
                manual_buffer.as_mut_ptr() as *mut T,
                manual_buffer.len(),
                manual_buffer.capacity(),
            )
        };

        Ok(final_vec)
    }

    // --- COLLECTION UTILITIES ---

    /// Returns the logical number of items in this container.
    pub fn len(&self) -> u64 {
        if let Ok(payload) = self.read_raw()
            && payload.len() >= 8
            && let Some(bytes) = payload.get(0..8).and_then(|s| s.try_into().ok())
        {
            return u64::from_le_bytes(bytes);
        }
        0
    }

    /// Checks if the container is empty.
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    /// Retrieves item at `index` using RLE arithmetic.
    ///
    /// This calculates which shard holds the item, loads ONLY that shard,
    /// and returns the specific item.
    pub fn get<T: DeserializeOwned>(&self, index: usize) -> Result<T> {
        let payload = self.read_raw()?;
        if payload.len() < 8 {
            return Err(ParcodeError::Format("Not a valid container".into()));
        }

        let runs_data = payload.get(8..).unwrap_or(&[]);
        let shard_runs: Vec<ShardRun> =
            bincode::serde::decode_from_slice(runs_data, bincode::config::standard())
                .map(|(obj, _)| obj)
                .map_err(|e| ParcodeError::Serialization(e.to_string()))?;

        let (target_shard_idx, index_in_shard) = self.resolve_rle_index(index, &shard_runs)?;

        let shard_node = self.get_child_by_index(target_shard_idx)?;
        let shard_data: Vec<T> = shard_node.decode()?;

        shard_data
            .into_iter()
            .nth(index_in_shard)
            .ok_or(ParcodeError::Internal("Shard index mismatch".into()))
    }

    /// Creates a streaming iterator over the collection.
    /// Memory usage is constant (size of one shard) regardless of total size.
    pub fn iter<T: DeserializeOwned>(self) -> Result<ChunkIterator<'a, T>> {
        let payload = self.read_raw()?;
        if payload.is_empty() && self.child_count == 0 {
            return Ok(ChunkIterator::empty(self));
        }
        if payload.len() < 8 {
            return Err(ParcodeError::Format("Not a valid container".into()));
        }

        let total_len = usize::try_from(u64::from_le_bytes(
            payload
                .get(0..8)
                .ok_or_else(|| ParcodeError::Format("Payload too short".into()))?
                .try_into()
                .map_err(|_| ParcodeError::Format("Failed to read total_len".into()))?,
        ))
        .map_err(|_| ParcodeError::Format("total_len exceeds usize range".into()))?;
        let runs_data = payload.get(8..).unwrap_or(&[]);
        let shard_runs: Vec<ShardRun> =
            bincode::serde::decode_from_slice(runs_data, bincode::config::standard())
                .map(|(obj, _)| obj)
                .map_err(|e| ParcodeError::Serialization(e.to_string()))?;

        Ok(ChunkIterator {
            container: self,
            shard_runs,
            total_items: total_len,
            current_global_idx: 0,
            current_shard_idx: 0,
            current_items_in_shard: Vec::new().into_iter(),
            _marker: PhantomData,
        })
    }

    // --- INTERNAL HELPERS ---

    /// Retrieves a child `ChunkNode` by its index in the footer.
    pub fn get_child_by_index(&self, index: usize) -> Result<Self> {
        if index >= self.child_count as usize {
            return Err(ParcodeError::Format("Child index out of bounds".into()));
        }
        let footer_start = usize::try_from(self.payload_end_offset)
            .map_err(|_| ParcodeError::Format("Offset exceeds usize range".into()))?;
        let entry_start = footer_start + (index * ChildRef::SIZE);
        let bytes = self
            .reader
            .mmap
            .get(entry_start..entry_start + ChildRef::SIZE)
            .ok_or_else(|| ParcodeError::Format("ChildRef index out of bounds".into()))?;

        let r = ChildRef::from_bytes(bytes)?;
        self.reader.get_chunk(r.offset, r.length)
    }

    /// Maps a global item index to a specific (`shard_index`, `internal_index`).
    fn resolve_rle_index(&self, global_index: usize, runs: &[ShardRun]) -> Result<(usize, usize)> {
        let mut current_base = 0;
        let mut shard_base = 0;

        for run in runs {
            let count = run.item_count as usize;
            let total_run = count * run.repeat as usize;

            if global_index < current_base + total_run {
                let offset = global_index - current_base;
                // Integer division gives logical shard, modulo gives index inside
                return Ok((shard_base + (offset / count), offset % count));
            }
            current_base += total_run;
            shard_base += run.repeat as usize;
        }
        Err(ParcodeError::Format("Index out of bounds".into()))
    }
}

// --- STREAMING ITERATOR ---

/// An iterator that loads shards on demand, allowing iteration over datasets
/// larger than available RAM.
///
/// It buffers only one shard at a time.
#[derive(Debug)]
pub struct ChunkIterator<'a, T> {
    container: ChunkNode<'a>,
    #[allow(dead_code)]
    shard_runs: Vec<ShardRun>, // Reserved for future skip logic
    total_items: usize,
    current_global_idx: usize,

    // State
    current_shard_idx: usize,
    current_items_in_shard: std::vec::IntoIter<T>,

    _marker: PhantomData<T>,
}

impl<'a, T> ChunkIterator<'a, T> {
    fn empty(node: ChunkNode<'a>) -> Self {
        Self {
            container: node,
            shard_runs: vec![],
            total_items: 0,
            current_global_idx: 0,
            current_shard_idx: 0,
            current_items_in_shard: Vec::new().into_iter(),
            _marker: PhantomData,
        }
    }
}

impl<'a, T: DeserializeOwned> Iterator for ChunkIterator<'a, T> {
    type Item = Result<T>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.current_global_idx >= self.total_items {
            return None;
        }

        // 1. Try to pull from current loaded shard
        if let Some(item) = self.current_items_in_shard.next() {
            self.current_global_idx += 1;
            return Some(Ok(item));
        }

        // 2. Buffer empty? Load next shard
        if self.current_shard_idx >= self.container.child_count as usize {
            return Some(Err(ParcodeError::Internal(
                "Iterator mismatch: runs out of shards".into(),
            )));
        }

        // Load logic
        let next_shard_res = self
            .container
            .get_child_by_index(self.current_shard_idx)
            .and_then(|node| node.decode::<Vec<T>>());

        match next_shard_res {
            Ok(items) => {
                self.current_items_in_shard = items.into_iter();
                self.current_shard_idx += 1;
                // Recursively call next to yield the first item of the new shard
                self.next()
            }
            Err(e) => Some(Err(e)),
        }
    }
}

===== src\rt.rs =====
// src/rt.rs

//! Runtime utilities for generated code (Macros).
//! Do not use directly.

use crate::error::Result;
use crate::format::ChildRef;
use crate::graph::{JobConfig, SerializationJob};
use crate::reader::ChunkNode;
use serde::de::DeserializeOwned;
use std::collections::HashMap;
use std::hash::Hash;
use std::marker::PhantomData;

// --- EXISTING CONFIG WRAPPER ---

/// Wrapper that injects configuration into an existing Job.
#[derive(Debug)]
pub struct ConfiguredJob<'a, J: ?Sized> {
    config: JobConfig,
    inner: Box<J>,
    _marker: PhantomData<&'a ()>,
}

impl<'a, J: SerializationJob<'a> + ?Sized> ConfiguredJob<'a, J> {
    pub fn new(inner: Box<J>, config: JobConfig) -> Self {
        Self {
            inner,
            config,
            _marker: PhantomData,
        }
    }
}

impl<'a, J: SerializationJob<'a> + ?Sized> SerializationJob<'a> for ConfiguredJob<'a, J> {
    fn execute(&self, children_refs: &[ChildRef]) -> Result<Vec<u8>> {
        self.inner.execute(children_refs)
    }

    fn estimated_size(&self) -> usize {
        self.inner.estimated_size()
    }

    fn config(&self) -> JobConfig {
        self.config
    }
}

// --- NEW LAZY MIRROR INFRASTRUCTURE ---

/// Trait implemented by types that support Lazy Mirroring.
///
/// This trait acts as a bridge between the original type `T` and its generated
/// lazy counterpart `T::Lazy`.
pub trait ParcodeLazyRef<'a>: Sized {
    /// The Mirror Type.
    /// For primitives, it is `ParcodePromise<'a, T>`.
    /// For structs deriving `ParcodeObject`, it is `StructNameLazy<'a>`.
    type Lazy;

    /// Creates the lazy view from a graph node.
    fn create_lazy(node: ChunkNode<'a>) -> Result<Self::Lazy>;
}

/// A terminal promise for a single value.
///
/// Use `.load()` to trigger deserialization.
#[derive(Debug, Clone)]
pub struct ParcodePromise<'a, T> {
    node: ChunkNode<'a>,
    _m: PhantomData<T>,
}

impl<'a, T: DeserializeOwned> ParcodePromise<'a, T> {
    /// Internal constructor.
    pub fn new(node: ChunkNode<'a>) -> Self {
        Self {
            node,
            _m: PhantomData,
        }
    }

    /// Loads the data from disk/memory.
    pub fn load(&self) -> Result<T> {
        self.node.decode()
    }
}

/// A promise for a collection (Vector).
///
/// Supports partial loading and random access via `.get(index)`.
/// Specialized lazy field for collections (Vec) allowing partial access.
#[derive(Debug, Clone)]
pub struct ParcodeCollectionPromise<'a, T> {
    node: ChunkNode<'a>,
    _m: PhantomData<T>,
}

impl<'a, T: DeserializeOwned + Send + Sync + 'a> ParcodeCollectionPromise<'a, T> {
    /// Internal constructor.
    pub fn new(node: ChunkNode<'a>) -> Self {
        Self {
            node,
            _m: PhantomData,
        }
    }

    /// Loads the entire collection into memory.
    pub fn load(&self) -> Result<Vec<T>> {
        self.node.decode_parallel_collection()
    }

    /// Retrieves a single item without loading the whole collection.
    /// Uses O(1) arithmetic navigation.
    pub fn get(&self, index: usize) -> Result<T> {
        self.node.get(index)
    }

    /// Returns a streaming iterator.
    pub fn iter(&self) -> Result<impl Iterator<Item = Result<T>> + 'a> {
        self.node.clone().iter()
    }
}

#[derive(Debug)]
pub struct ParcodeMapPromise<'a, K, V> {
    node: ChunkNode<'a>,
    _m: PhantomData<(K, V)>,
}

impl<'a, K, V> ParcodeMapPromise<'a, K, V>
where
    K: Hash + Eq + DeserializeOwned,
    V: DeserializeOwned,
{
    /// Constructor interno.
    pub fn new(node: ChunkNode<'a>) -> Self {
        Self {
            node,
            _m: PhantomData,
        }
    }

    /// Loads full map by iterating all shards.
    pub fn load(&self) -> Result<HashMap<K, V>> {
        // 1. Leer número de shards del contenedor
        let container_payload = self.node.read_raw()?;
        let num_shards = if container_payload.len() >= 4 {
            u32::from_le_bytes(container_payload[0..4].try_into().unwrap())
        } else {
            0
        };

        let mut map = HashMap::new();
        if num_shards == 0 {
            return Ok(map);
        }

        // 2. Iterar Shards
        // Usamos children() que devuelve Vec<ChunkNode>
        let shards = self.node.children()?;
        for shard in shards {
            let payload = shard.read_raw()?;
            if payload.len() < 8 {
                continue;
            }

            // Parsear header SOA
            let count = u32::from_le_bytes(payload[0..4].try_into().unwrap()) as usize;

            // Layout: Count(4) + Padding(4) + Hashes(8*N) + Offsets(4*N) + Data
            let offsets_start = 8 + (count * 8);
            let data_start = offsets_start + (count * 4);

            // Leer offsets para iterar datos
            let offsets_bytes = &payload[offsets_start..data_start];

            for i in 0..count {
                let off_bytes = &offsets_bytes[i * 4..(i + 1) * 4];
                let offset = u32::from_le_bytes(off_bytes.try_into().unwrap()) as usize;
                let data_slice = &payload[data_start + offset..];

                // Deserializar par (K, V)
                let (k, v): (K, V) =
                    bincode::serde::decode_from_slice(data_slice, bincode::config::standard())
                        .map_err(|e| crate::ParcodeError::Serialization(e.to_string()))?
                        .0;

                map.insert(k, v);
            }
        }
        Ok(map)
    }

    pub fn get(&self, key: &K) -> Result<Option<V>> {
        // 1. Leer Container Payload (Num Shards)
        let container_payload = self.node.read_raw()?;
        if container_payload.len() < 4 {
            return Ok(None);
        } // Vacío
        let num_shards = u32::from_le_bytes(container_payload[0..4].try_into().unwrap());

        // 2. Hash & Select Shard
        let target_hash = crate::map::hash_key(key);
        let shard_idx = (target_hash as usize) % (num_shards as usize);

        // 3. Load Shard
        let shard = self.node.get_child_by_index(shard_idx)?;
        let payload = shard.read_raw()?;

        if payload.len() < 8 {
            return Ok(None);
        } // Empty shard

        let count = u32::from_le_bytes(payload[0..4].try_into().unwrap()) as usize;
        // Skip 4 bytes padding -> Offset 8

        let hashes_start = 8;
        let hashes_end = hashes_start + (count * 8);
        let offsets_start = hashes_end;
        let data_start = offsets_start + (count * 4);

        // 4. Fast Scan (SIMD Optimized via chunks_exact)
        let hashes_slice = &payload[hashes_start..hashes_end];

        for (i, chunk) in hashes_slice.chunks_exact(8).enumerate() {
            let h = u64::from_le_bytes(chunk.try_into().unwrap());

            if h == target_hash {
                // Candidato. Verificar.
                let offset_bytes = &payload[offsets_start + (i * 4)..];
                let offset = u32::from_le_bytes(offset_bytes[0..4].try_into().unwrap()) as usize;

                let data_slice = &payload[data_start + offset..];

                // Deserializar (K, V)
                // Usamos bincode::deserialize_from slice. Bincode sabe cuándo parar.
                let (stored_key, stored_val): (K, V) =
                    bincode::serde::decode_from_slice(data_slice, bincode::config::standard())
                        .map_err(|e| crate::ParcodeError::Serialization(e.to_string()))?
                        .0;

                if &stored_key == key {
                    return Ok(Some(stored_val));
                }
                // Si no coincide, es una colisión de hash (raro). Seguimos buscando.
            }
        }

        Ok(None)
    }
}

impl<'a, K, V> ParcodeLazyRef<'a> for HashMap<K, V>
where
    K: Hash + Eq + DeserializeOwned + Send + Sync + 'static,
    V: DeserializeOwned + Send + Sync + 'static,
{
    type Lazy = ParcodeMapPromise<'a, K, V>;
    fn create_lazy(node: ChunkNode<'a>) -> Result<Self::Lazy> {
        Ok(ParcodeMapPromise::new(node))
    }
}

// --- BLANKET IMPLEMENTATIONS FOR PRIMITIVES ---

macro_rules! impl_lazy_primitive {
    ($($t:ty),*) => {
        $(
            impl<'a> ParcodeLazyRef<'a> for $t {
                type Lazy = ParcodePromise<'a, $t>;
                fn create_lazy(node: ChunkNode<'a>) -> Result<Self::Lazy> {
                    Ok(ParcodePromise::new(node))
                }
            }
        )*
    }
}

impl_lazy_primitive!(u8, u16, u32, u64, i8, i16, i32, i64, f32, f64, bool, String);

// --- BLANKET IMPLEMENTATION FOR VECTORS ---

impl<'a, T: DeserializeOwned + Send + Sync + 'static> ParcodeLazyRef<'a> for Vec<T> {
    type Lazy = ParcodeCollectionPromise<'a, T>;
    fn create_lazy(node: ChunkNode<'a>) -> Result<Self::Lazy> {
        Ok(ParcodeCollectionPromise::new(node))
    }
}

===== src\visitor.rs =====
//! Defines the `ParcodeVisitor` trait for graph construction.
//!
//! This module allows types to define how they should be split into nodes
//! in the `TaskGraph`.

//use crate::graph::core::TaskGraph;
//use crate::graph::id::ChunkId;
//use crate::graph::job::SerializationJob;
use crate::graph::{ChunkId, JobConfig, SerializationJob, TaskGraph};

/// A trait for types that can be structurally visited to build a Parcode `TaskGraph`.
///
/// This is distinct from `serde::Serialize`. Instead of writing bytes,
/// this trait writes *Nodes* into the graph.
pub trait ParcodeVisitor {
    /// Visits the object and populates the graph.
    ///
    /// # Lifetimes
    /// * `'a`: The lifetime of the graph. `Self` must outlive `'a` if we want to
    ///   store references to `self` inside the graph (Zero-Copy).
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    );

    /// Helper method to create the specific Job for this type.
    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a>;
}

// Example implementation for primitives (leaves in the graph logic,
// usually inlined, but here shown if they were chunks).
// In reality, primitives are almost always part of a parent chunk payload.

===== src\visitor_impls.rs =====
//! Implementation of `ParcodeVisitor` for standard Rust collections.
//!
//! # Sharding Strategy V3: Adaptive and Concurrency-Aware
//!
//! This module decides how to split a `Vec<T>` into shards. The decision is based on:
//! 1.  **Size in Bytes:** We aim for ~128KB per chunk to optimize SSD throughput and compression.
//! 2.  **CPU Saturation:** We ensure enough chunks are created to feed all available cores,
//!     even if it means creating smaller chunks (down to a floor of 4KB).
//! 3.  **Real Data Sampling:** We measure the actual serialization size of a sample to handle
//!     heap-allocated data (like `Vec<String>`) accurately.

use crate::error::{ParcodeError, Result};
use crate::format::ChildRef;
use crate::graph::{ChunkId, JobConfig, SerializationJob, TaskGraph};
use crate::map::{MapShardJob, hash_key};
use crate::visitor::ParcodeVisitor;
use serde::de::DeserializeOwned;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::hash::Hash;

// --- TUNING CONSTANTS ---

/// Ideal size for a chunk on disk. Optimized for SSD throughput.
const TARGET_SHARD_SIZE_BYTES: u64 = 128 * 1024; // 128 KB

/// Absolute minimum size to avoid excessive OS/graph overhead.
const MIN_SHARD_SIZE_BYTES: u64 = 4 * 1024; // 4 KB

/// CPU core multiplier. If we have 8 cores, we want at least 32 tasks
/// to allow effective "work-stealing" and load balancing in Rayon.
const TASKS_PER_CORE: usize = 4;

// --- Internal Metadata Structures ---

/// RLE (Run-Length Encoding) structure to map logical indices to physical shards.
#[derive(Clone, Serialize, Deserialize, Debug)]
struct ShardRun {
    item_count: u32,
    repeat: u32,
}

/// The job that serializes the Vector Container Node.
/// Contains only metadata (RLE table and total length), not the data itself.
#[derive(Clone)]
struct VecContainerJob {
    shard_runs: Vec<ShardRun>,
    total_items: u64,
}

// ContainerJob owns its metadata, so it works for any lifetime 'a
impl<'a> SerializationJob<'a> for VecContainerJob {
    fn execute(&self, _children_refs: &[ChildRef]) -> Result<Vec<u8>> {
        let mut buffer = Vec::new();
        buffer.extend_from_slice(&self.total_items.to_le_bytes());
        let runs_bytes =
            bincode::serde::encode_to_vec(&self.shard_runs, bincode::config::standard())
                .map_err(|e| ParcodeError::Serialization(e.to_string()))?;
        buffer.extend_from_slice(&runs_bytes);
        Ok(buffer)
    }

    fn estimated_size(&self) -> usize {
        8 + (self.shard_runs.len() * 8)
    }
}

/// The job that serializes a real data Shard.
/// Contains a subset of the original vector (`data`).
#[derive(Clone)]
struct VecShardJob<'a, T> {
    data: &'a [T],
}

impl<'a, T> SerializationJob<'a> for VecShardJob<'a, T>
where
    T: Serialize + Send + Sync + 'static,
{
    fn execute(&self, _children_refs: &[ChildRef]) -> Result<Vec<u8>> {
        // Serialize the SLICE directly. No cloning occurred during graph build.
        bincode::serde::encode_to_vec(self.data, bincode::config::standard())
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }

    fn estimated_size(&self) -> usize {
        std::mem::size_of_val(self.data)
    }
}

impl<'a, T> SerializationJob<'a> for &T
where
    T: SerializationJob<'a> + ?Sized,
{
    fn execute(&self, children_refs: &[ChildRef]) -> Result<Vec<u8>> {
        (**self).execute(children_refs)
    }

    fn estimated_size(&self) -> usize {
        (**self).estimated_size()
    }

    fn config(&self) -> JobConfig {
        (**self).config()
    }
}

// --- Visitor Implementation ---

impl<T> ParcodeVisitor for Vec<T>
where
    T: ParcodeVisitor + Clone + Send + Sync + 'static + Serialize,
{
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        let total_len = self.len();
        let items_per_shard;

        // --- PHASE 1: SHARDING STRATEGY CALCULATION ---
        if total_len == 0 {
            items_per_shard = 1;
        } else {
            // 1. Measure data cost (Sampling)
            // We take up to 8 elements to estimate the real size (useful for Strings/Heap).
            let sample_count = total_len.min(8);
            let sample_slice = self.get(0..sample_count).unwrap_or(&[]);

            let sample_size_bytes =
                match bincode::serde::encode_to_vec(sample_slice, bincode::config::standard()) {
                    Ok(vec) => vec.len() as u64,
                    Err(_) => 0,
                };

            // Calculate average bytes per item (minimum 1 byte to avoid div by zero)
            let avg_item_size = if sample_size_bytes > 0 {
                (sample_size_bytes / sample_count as u64).max(1)
            } else {
                (std::mem::size_of::<T>() as u64).max(1)
            };

            // 2. Calculate Strategies

            // Strategy A: Optimized for I/O (Fill 128KB chunks)
            let count_by_io = usize::try_from((TARGET_SHARD_SIZE_BYTES / avg_item_size).max(1))
                .unwrap_or(usize::MAX);

            // Strategy B: Optimized for CPU (Fill cores)
            // We want enough tasks to keep Rayon busy.
            let num_cpus = std::thread::available_parallelism()
                .map(|n| n.get())
                .unwrap_or(1);
            let target_parallel_chunks = num_cpus * TASKS_PER_CORE;
            let count_by_cpu = (total_len / target_parallel_chunks).max(1);

            // 3. Strategy Fusion
            // We prefer more chunks (CPU) unless they are ridiculously small.
            let candidate_count = count_by_io.min(count_by_cpu);

            // Verify physical size of the candidate
            let estimated_chunk_size = candidate_count as u64 * avg_item_size;

            if estimated_chunk_size < MIN_SHARD_SIZE_BYTES {
                // Too small. Scale to meet the 4KB minimum.
                items_per_shard = usize::try_from((MIN_SHARD_SIZE_BYTES / avg_item_size).max(1))
                    .unwrap_or(usize::MAX);
            } else {
                items_per_shard = candidate_count;
            }
        }

        // --- PHASE 2: GRAPH CONSTRUCTION ---

        // Generate slices (views) of the data without copying memory yet.
        let chunks: Vec<&[T]> = self.chunks(items_per_shard).collect();

        // Build RLE metadata
        let mut shard_runs: Vec<ShardRun> = Vec::new();
        if !chunks.is_empty() {
            let mut current_run = ShardRun {
                item_count: u32::try_from(chunks.first().map(|c| c.len()).unwrap_or(0))
                    .unwrap_or(u32::MAX),
                repeat: 0,
            };
            for chunk in &chunks {
                let len = u32::try_from(chunk.len()).unwrap_or(u32::MAX);
                if len == current_run.item_count {
                    current_run.repeat += 1;
                } else {
                    shard_runs.push(current_run);
                    current_run = ShardRun {
                        item_count: len,
                        repeat: 1,
                    };
                }
            }
            shard_runs.push(current_run);
        }

        // 1. Create and Register the Container Node
        let container_inner = Box::new(VecContainerJob {
            shard_runs,
            total_items: self.len() as u64,
        });

        // Apply configuration (override) to the container if it exists.
        // If the user requests LZ4, the container is also marked as LZ4 (even if small).
        let container_job: Box<dyn SerializationJob<'a> + 'a> = if let Some(cfg) = config_override {
            Box::new(crate::rt::ConfiguredJob::new(container_inner, cfg))
        } else {
            container_inner
        };

        let my_id = graph.add_node(container_job);
        if let Some(pid) = parent_id {
            graph.link_parent_child(pid, my_id);
        }

        if self.is_empty() {
            return;
        }

        // 2. Create Shard Nodes (Children)
        for chunk_slice in chunks {
            // ZERO-COPY: We wrap the slice reference directly.
            let shard_inner = Box::new(VecShardJob { data: chunk_slice });

            // CONFIGURATION PROPAGATION:
            // It is critical to apply the vector configuration (e.g., LZ4 Compression) to the Shards,
            // as this is where 99% of the bytes reside.
            let shard_job: Box<dyn SerializationJob<'a> + 'a> = if let Some(cfg) = config_override {
                Box::new(crate::rt::ConfiguredJob::new(shard_inner, cfg))
            } else {
                shard_inner
            };

            let shard_id = graph.add_node(shard_job);
            graph.link_parent_child(my_id, shard_id);

            // Recursion to individual items.
            // Note: We pass 'None' as config override.
            // Reason: Items T are serialized within the Shard payload using Bincode.
            // They are not independent graph nodes (unless T explicitly creates sub-nodes).
            // If T is a complex struct, its own configuration (via Macro) will dictate how it behaves.
            for item in chunk_slice {
                // Recursion propagates the graph reference
                item.visit(graph, Some(shard_id), None);
            }
        }
    }

    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        let inner = Box::new(VecContainerJob {
            shard_runs: Vec::new(),
            total_items: 0,
        });
        if let Some(cfg) = config_override {
            Box::new(crate::rt::ConfiguredJob::new(inner, cfg))
        } else {
            inner
        }
    }
}

// --- Implementation for Primitives ---

#[derive(Clone)]
struct PrimitiveJob<T>(T);

// Primitives own their data (copy), so they are valid for any lifetime 'a
impl<'a, T> SerializationJob<'a> for PrimitiveJob<T>
where
    T: Serialize + Send + Sync + Clone + 'static,
{
    fn execute(&self, _: &[ChildRef]) -> Result<Vec<u8>> {
        bincode::serde::encode_to_vec(&self.0, bincode::config::standard())
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }
    fn estimated_size(&self) -> usize {
        std::mem::size_of::<T>()
    }
}

// --- SOPORTE PARA HASHMAP ---

/// El trabajo que serializa el contenedor de un Mapa.
/// Su único payload es el número de shards (u32), necesario para que el lector
/// calcule el módulo del hash correctamente.
#[derive(Clone)]
struct MapContainerJob {
    num_shards: u32,
}

impl<'a> SerializationJob<'a> for MapContainerJob {
    fn execute(&self, _children_refs: &[ChildRef]) -> Result<Vec<u8>> {
        // Escribimos simplemente el número de shards en Little Endian.
        Ok(self.num_shards.to_le_bytes().to_vec())
    }

    fn estimated_size(&self) -> usize {
        4
    }
}

impl<K, V> ParcodeVisitor for HashMap<K, V>
where
    K: Serialize + DeserializeOwned + Hash + Eq + Send + Sync + Clone + 'static,
    V: Serialize + DeserializeOwned + Send + Sync + Clone + 'static,
{
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        // Detectamos si se solicitó la optimización de mapa
        let is_map_optimized = config_override.map(|c| c.is_map).unwrap_or(false);
        let total_items = self.len();

        if !is_map_optimized || total_items < 200 {
            // Estrategia Estándar: Blob único.
            // Si parent_id es None, somos raíz -> Crear nodo.
            // Si parent_id es Some, somos un hijo chunkable -> Crear nodo y enlazar.
            // (La macro solo llama a visit con Some si el campo es chunkable).

            let job = if total_items != 0 {
                self.create_job(config_override)
            } else {
                Box::new(MapContainerJob { num_shards: 0 })
            };
            let my_id = graph.add_node(job);

            if let Some(p) = parent_id {
                graph.link_parent_child(p, my_id);
            }
            return;
        }

        //// Estrategia Optimizada (Bucketing + Micro-Index)
        //if total_items == 0 {
        //    // Mapa vacío -> Contenedor con 0 shards
        //    let container = Box::new(MapContainerJob { num_shards: 0 });
        //    let id = graph.add_node(container);
        //    if let Some(p) = parent_id {
        //        graph.link_parent_child(p, id);
        //    }
        //    return;
        //}

        // Heurística de Sharding:
        // Buscamos que cada shard tenga un tamaño razonable para búsqueda lineal rápida.
        // 500-1000 items por bucket suele ser un buen balance para evitar colisiones
        // y mantener el micro-índice en caché L1/L2.
        // Limitamos a 256 shards por defecto para no explotar el grafo en mapas gigantes
        // (aunque el formato soporta más).
        let target_items_per_bucket = 2000;
        let num_shards = (total_items / target_items_per_bucket).max(1).min(1024);

        // Fase 1: Distribución (Bucketing)
        // Recolectamos referencias (&K, &V) para no clonar la memoria.
        let mut buckets = vec![Vec::new(); num_shards];

        for (k, v) in self {
            let h = hash_key(k);
            let idx = (h as usize) % num_shards;
            buckets[idx].push((k, v));
        }

        // Fase 2: Creación de Nodos
        // Nodo Contenedor
        let container_inner = Box::new(MapContainerJob {
            num_shards: num_shards as u32,
        });
        // Si hay compresión global, la aplicamos al contenedor (aunque es pequeño).
        let container_job: Box<dyn SerializationJob<'a> + 'a> = if let Some(cfg) = config_override {
            Box::new(crate::rt::ConfiguredJob::new(container_inner, cfg))
        } else {
            container_inner
        };

        let my_id = graph.add_node(container_job);
        if let Some(p) = parent_id {
            graph.link_parent_child(p, my_id);
        }

        // Nodos Shard
        for bucket in buckets {
            if bucket.is_empty() {
                continue;
            } // Optimizacion: No crear shards vacíos

            let shard_inner = Box::new(MapShardJob { items: bucket });

            // Aplicamos la misma configuración (compresión) a los shards
            let shard_job: Box<dyn SerializationJob<'a> + 'a> = if let Some(cfg) = config_override {
                Box::new(crate::rt::ConfiguredJob::new(shard_inner, cfg))
            } else {
                shard_inner
            };

            let child_id = graph.add_node(shard_job);
            graph.link_parent_child(my_id, child_id);
        }
    }

    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        // Fallback para cuando se usa como primitivo (sin estrategia de mapa)
        let base_job = Box::new(PrimitiveJob(self.clone()));
        if let Some(cfg) = config_override {
            Box::new(crate::rt::ConfiguredJob::new(base_job, cfg))
        } else {
            base_job
        }
    }
}

impl<T: ParcodeVisitor> ParcodeVisitor for &T {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        (**self).visit(graph, parent_id, config_override);
    }

    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        (**self).create_job(config_override)
    }
}

/// Macro to implement `ParcodeVisitor` for primitive types massively.
macro_rules! impl_primitive_visitor {
    ($($t:ty),*) => {
        $(
            impl ParcodeVisitor for $t {
                fn visit<'a>(&'a self, graph: &mut TaskGraph<'a>, parent_id: Option<ChunkId>, config_override: Option<JobConfig>) {
                    if parent_id.is_none() {
                        let job = self.create_job(config_override);
                        graph.add_node(job);
                    }
                }

                fn create_job<'a>(&'a self, config_override: Option<JobConfig>) -> Box<dyn SerializationJob<'a> + 'a> {
                    let base_job = Box::new(PrimitiveJob(self.clone()));
                    if let Some(cfg) = config_override {
                        Box::new(crate::rt::ConfiguredJob::new(base_job, cfg))
                    } else {
                        base_job
                    }
                }
            }
        )*
    }
}

impl_primitive_visitor!(
    u8, u16, u32, u64, u128, i8, i16, i32, i64, i128, f32, f64, bool, String
);

===== tests\integration_tests.rs =====
// ===== tests\integration_tests.rs =====
//! Suite de pruebas de integración para Parcode.

use parcode::{
    Parcode, ParcodeError, ParcodeReader, Result,
    format::ChildRef,
    graph::{ChunkId, JobConfig, SerializationJob, TaskGraph},
    reader::{ChunkNode, ParcodeNative},
    visitor::ParcodeVisitor,
};
use serde::{Deserialize, Serialize};
use std::fs::{File, OpenOptions};
use std::io::Write;
use tempfile::NamedTempFile;

// --- INFRAESTRUCTURA DE PRUEBA (Mocks y Structs) ---

#[derive(Clone, Serialize, Deserialize, Debug, PartialEq)]
struct TestUser {
    id: u64,
    username: String,
    active: bool,
}

// Implementación manual de ParcodeVisitor (lo que haría la macro)
impl ParcodeVisitor for TestUser {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        // 1. Crear trabajo
        let job = self.create_job(config_override);
        let my_id = graph.add_node(job);
        // 2. Ligar al padre
        if let Some(pid) = parent_id {
            graph.link_parent_child(pid, my_id);
        }
    }
    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        let base = Box::new(self.clone());
        if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(base, cfg))
        } else {
            base
        }
    }
}

// Implementación manual de SerializationJob
impl SerializationJob<'_> for TestUser {
    fn execute(&self, _children: &[ChildRef]) -> Result<Vec<u8>> {
        bincode::serde::encode_to_vec(self, bincode::config::standard())
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }
    fn estimated_size(&self) -> usize {
        100
    }
}

impl ParcodeNative for TestUser {
    fn from_node(node: &ChunkNode<'_>) -> Result<Self> {
        node.decode::<Self>()
    }
}

// --- WRAPPER PARA VECTORES PERSONALIZADOS ---

#[derive(Clone, Serialize, Deserialize, Debug, PartialEq)]
struct UserDirectory {
    region: String,
    users: Vec<TestUser>,
}

impl ParcodeVisitor for UserDirectory {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        // 1. Nodo contenedor para 'region'
        #[derive(Serialize)]
        struct Header {
            region: String,
        }
        let header = Header {
            region: self.region.clone(),
        };

        struct HeaderJob(Header);
        impl SerializationJob<'_> for HeaderJob {
            fn execute(&self, _: &[ChildRef]) -> Result<Vec<u8>> {
                bincode::serde::encode_to_vec(&self.0, bincode::config::standard())
                    .map_err(|e| ParcodeError::Serialization(e.to_string()))
            }
            fn estimated_size(&self) -> usize {
                50
            }
        }

        let header_job_base = Box::new(HeaderJob(header));
        // Aplicamos config si existe
        let header_job: Box<dyn SerializationJob<'_>> = if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(header_job_base, cfg))
        } else {
            header_job_base
        };

        let my_id = graph.add_node(header_job);
        if let Some(pid) = parent_id {
            graph.link_parent_child(pid, my_id);
        }

        // 2. DELEGAR AL VEC
        // Aquí propagamos None, pero podríamos propagar config_override si quisiéramos que la config
        // del padre afectara a los hijos users.
        self.users.visit(graph, Some(my_id), None);
    }

    fn create_job(&self, _config_override: Option<JobConfig>) -> Box<dyn SerializationJob<'_>> {
        panic!("Not used in root read for UserDirectory mock")
    }
}

// --- MOCK DATA STRUCTURES (Game Level) ---

#[derive(Clone, Serialize, Deserialize, Debug, PartialEq)]
struct ZoneList(Vec<GameZone>);

#[derive(Clone, Serialize, Deserialize, Debug, PartialEq)]
struct GameLevel {
    id: u32,
    name: String,
    zones: ZoneList,
}

#[derive(Clone, Serialize, Deserialize, Debug, PartialEq)]
struct GameZone {
    zone_id: u32,
    data: Vec<u8>,
}

// --- IMPLEMENTACIONES JUEGO ---

impl ParcodeVisitor for GameLevel {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        let job = self.create_job(config_override);
        let my_id = graph.add_node(job);
        if let Some(pid) = parent_id {
            graph.link_parent_child(pid, my_id);
        }

        self.zones.visit(graph, Some(my_id), None);
    }
    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        let base = Box::new(self.clone());
        if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(base, cfg))
        } else {
            base
        }
    }
}

impl SerializationJob<'_> for GameLevel {
    fn execute(&self, _children: &[ChildRef]) -> Result<Vec<u8>> {
        let local_data = (&self.id, &self.name);
        bincode::serde::encode_to_vec(&local_data, bincode::config::standard())
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }
    fn estimated_size(&self) -> usize {
        100
    }
}

impl ParcodeVisitor for ZoneList {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        let job = self.create_job(config_override);
        let my_id = graph.add_node(job);
        if let Some(pid) = parent_id {
            graph.link_parent_child(pid, my_id);
        }

        for zone in &self.0 {
            zone.visit(graph, Some(my_id), None);
        }
    }
    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        let base = Box::new(self.clone());
        if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(base, cfg))
        } else {
            base
        }
    }
}

impl SerializationJob<'_> for ZoneList {
    fn execute(&self, _children: &[ChildRef]) -> Result<Vec<u8>> {
        Ok(Vec::new())
    }
    fn estimated_size(&self) -> usize {
        0
    }
}

impl ParcodeVisitor for GameZone {
    fn visit<'a>(
        &'a self,
        graph: &mut TaskGraph<'a>,
        parent_id: Option<ChunkId>,
        config_override: Option<JobConfig>,
    ) {
        let job = self.create_job(config_override);
        let my_id = graph.add_node(job);
        if let Some(pid) = parent_id {
            graph.link_parent_child(pid, my_id);
        }
    }
    fn create_job<'a>(
        &'a self,
        config_override: Option<JobConfig>,
    ) -> Box<dyn SerializationJob<'a> + 'a> {
        let base = Box::new(self.clone());
        if let Some(cfg) = config_override {
            Box::new(parcode::rt::ConfiguredJob::new(base, cfg))
        } else {
            base
        }
    }
}

impl SerializationJob<'_> for GameZone {
    fn execute(&self, _children: &[ChildRef]) -> Result<Vec<u8>> {
        bincode::serde::encode_to_vec(self, bincode::config::standard())
            .map_err(|e| ParcodeError::Serialization(e.to_string()))
    }
    fn estimated_size(&self) -> usize {
        self.data.len()
    }
}

// --- TESTS ---

#[test]
fn test_primitive_lifecycle() -> Result<()> {
    let user = TestUser {
        id: 101,
        username: "satoshi".into(),
        active: true,
    };

    let file = NamedTempFile::new()?;

    // WRITE
    Parcode::save(file.path(), &user)?;

    // READ
    let loaded_user: TestUser = Parcode::read(file.path())?;

    assert_eq!(user, loaded_user);
    Ok(())
}

#[test]
fn test_massive_vector_sharding() -> Result<()> {
    let count = 100_000;
    let data: Vec<u64> = (0..count).map(|i| i as u64).collect();

    println!("Generating {} items (~{} KB)...", count, (count * 8) / 1024);

    let file = NamedTempFile::new()?;
    Parcode::save(file.path(), &data)?;

    let loaded_data: Vec<u64> = Parcode::read(file.path())?;

    assert_eq!(data.len(), loaded_data.len());
    assert_eq!(data, loaded_data);

    let reader = ParcodeReader::open(file.path())?;
    let root = reader.root()?;
    let shards = root.children()?;
    println!("Shards created: {}", shards.len());
    assert!(
        shards.len() > 1,
        "El sistema debería haber fragmentado el vector"
    );

    Ok(())
}

#[test]
fn test_nested_structures() -> Result<()> {
    let dir = UserDirectory {
        region: "EU-West".to_string(),
        users: vec![
            TestUser {
                id: 1,
                username: "a".into(),
                active: true,
            },
            TestUser {
                id: 2,
                username: "b".into(),
                active: false,
            },
            TestUser {
                id: 3,
                username: "c".into(),
                active: true,
            },
        ],
    };

    let file = NamedTempFile::new()?;
    Parcode::save(file.path(), &dir)?;

    let reader = ParcodeReader::open(file.path())?;
    let root = reader.root()?;

    #[derive(Deserialize)]
    struct Header {
        region: String,
    }
    let header: Header = root.decode()?;
    assert_eq!(header.region, "EU-West");

    let children = root.children()?;
    assert_eq!(children.len(), 1);

    let vec_container = &children[0];
    let loaded_users: Vec<TestUser> = vec_container.decode_parallel_collection()?;

    assert_eq!(dir.users, loaded_users);

    Ok(())
}

#[test]
fn test_random_access_logic() -> Result<()> {
    let count = 50_000;
    let data: Vec<u64> = (0..count).map(|i| i as u64 * 10).collect();

    let file = NamedTempFile::new()?;
    Parcode::save(file.path(), &data)?;

    let reader = ParcodeReader::open(file.path())?;
    let root = reader.root()?;

    let val_0: u64 = root.get(0)?;
    let val_mid: u64 = root.get(25_000)?;
    let val_last: u64 = root.get(49_999)?;

    assert_eq!(val_0, 0);
    assert_eq!(val_mid, 250_000);
    assert_eq!(val_last, 499_990);

    let err = root.get::<u64>(50_001);
    assert!(err.is_err());

    Ok(())
}

#[test]
fn test_corruption_and_errors() -> Result<()> {
    let file = NamedTempFile::new()?;
    let path = file.path().to_owned();

    {
        let _f = File::create(&path)?;
    }
    let res = ParcodeReader::open(&path);
    assert!(matches!(res, Err(ParcodeError::Format(_))));

    {
        let mut f = File::create(&path)?;
        let junk = vec![0u8; 100];
        f.write_all(&junk)?;
    }
    let res = ParcodeReader::open(&path);
    if let Err(ParcodeError::Format(msg)) = res {
        assert!(msg.contains("Magic"));
    } else {
        assert!(false, "No detectó magic bytes inválidos");
    }

    let user = TestUser {
        id: 1,
        username: "test".into(),
        active: true,
    };
    Parcode::save(&path, &user)?;

    let len = std::fs::metadata(&path)?.len();
    let f = OpenOptions::new().write(true).open(&path)?;
    f.set_len(len / 2)?;

    let res = ParcodeReader::open(&path);
    assert!(res.is_err());

    Ok(())
}

===== tests\lazy_tests.rs =====
// tests/lazy_test.rs

#![allow(missing_docs)]

use parcode::{Parcode, ParcodeObject, ParcodeReader};
use serde::{Deserialize, Serialize};
use tempfile::NamedTempFile;

// --- ESTRUCTURAS ANIDADAS ---

#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, ParcodeObject)]
struct Config {
    version: u32,
    #[parcode(chunkable)] // Nodo hijo
    blob: Vec<u8>,
}

#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, ParcodeObject)]
struct Level {
    id: u64,
    name: String,

    #[parcode(chunkable)]
    config: Config, // Otro ParcodeObject

    #[parcode(chunkable)]
    geometry: Vec<u32>, // Colección
}

#[test]
fn test_lazy_mirror_access() {
    let config = Config {
        version: 2,
        blob: vec![0xAA; 1024],
    };
    let level = Level {
        id: 101,
        name: "LazyZone".into(),
        config: config.clone(),
        geometry: (0..10_000).collect(),
    };

    let file = NamedTempFile::new().unwrap();
    Parcode::save(file.path(), &level).unwrap();

    let reader = ParcodeReader::open(file.path()).unwrap();

    // 1. Obtener el Espejo Lazy (NO carga los hijos chunkables)
    let lazy_level = reader.read_lazy::<Level>().unwrap();

    // 2. Validar Locales (Acceso directo, ya leídos)
    assert_eq!(lazy_level.id, 101);
    assert_eq!(lazy_level.name, "LazyZone");

    // 3. Validar Recursividad Lazy (Navegación sin carga total)
    // lazy_level.config es ConfigLazy
    // lazy_level.config.version es campo local de ConfigLazy (leído eager del chunk de config)
    // Para acceder a config, primero debemos "entrar" en su chunk.
    // La macro actual genera un método create_lazy que lee el payload.
    // PERO: El campo `config` en `LevelLazy` es de tipo `ConfigLazy`.
    // ¿Cuándo se instanció `ConfigLazy`?
    // En `Level::create_lazy` se llamó a `Config::create_lazy(child_node)`.
    // Por tanto, el payload de Config (que contiene version) YA SE LEYÓ.

    // Acceso directo a campo local del hijo:
    assert_eq!(lazy_level.config.version, 2);

    // 4. Validar Carga de Hoja Profunda
    // blob es ParcodeCollectionPromise<u8> dentro de config
    let loaded_blob = lazy_level.config.blob.load().unwrap();
    assert_eq!(loaded_blob.len(), 1024);
    assert_eq!(loaded_blob[0], 0xAA);

    // 5. Validar Acceso Parcial a Colección
    // geometry es ParcodeCollectionPromise<u32>
    let val = lazy_level.geometry.get(5000).unwrap();
    assert_eq!(val, 5000);
}

===== tests\macro_test.rs =====
#![allow(missing_docs)]

use parcode::{Parcode, ParcodeObject};
use serde::{Deserialize, Serialize};
use tempfile::NamedTempFile;

// Definición Ergonómica V3
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize, ParcodeObject)]
struct LevelState {
    id: u32,      // Local (Bincode directo)
    name: String, // Local

    #[parcode(chunkable)] // Automatic settings
    indices: Vec<u64>,
    #[cfg(feature = "lz4_flex")]
    #[parcode(chunkable, compression = "lz4")] // Explicit LZ4
    assets: Vec<u8>,
}

#[test]
fn test_macro_ergonomics() {
    let level = LevelState {
        id: 42,
        name: "Dungeon_01".into(),
        indices: (0..10_000).map(|i| i * 2).collect(),
        #[cfg(feature = "lz4_flex")]
        assets: vec![0xAA; 200_000], // 200KB -> Forzará sharding + LZ4
    };

    let file = NamedTempFile::new().unwrap();

    // 1. Guardar (La macro se encarga de todo el grafo)
    Parcode::save(file.path(), &level).unwrap();

    // 2. Leer (La macro se encarga de la reconstrucción)
    let loaded: LevelState = Parcode::read(file.path()).unwrap();

    assert_eq!(level, loaded);
}

===== tests\map_test.rs =====
// tests/map_test.rs

#![allow(missing_docs)]

use parcode::{Parcode, ParcodeObject, ParcodeReader};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tempfile::NamedTempFile;

// Estructura que usa el modo mapa optimizado
#[derive(Serialize, Deserialize, ParcodeObject)]
struct UserDatabase {
    id: u32,
    #[parcode(map, compression = "lz4")] // Activamos modo Mapa y LZ4
    users: HashMap<String, UserProfile>,
}

#[derive(Serialize, Deserialize, Clone, Debug, PartialEq)]
struct UserProfile {
    level: u32,
    score: u64,
}

#[test]
fn test_optimized_map_access() {
    // 1. Generar Datos (Suficientes para provocar sharding real)
    let mut users = HashMap::new();
    for i in 0..5000 {
        users.insert(
            format!("user_{}", i),
            UserProfile {
                level: i % 100,
                score: i as u64 * 10,
            },
        );
    }

    let db = UserDatabase {
        id: 1,
        users: users.clone(),
    };

    let file = NamedTempFile::new().unwrap();
    Parcode::save(file.path(), &db).unwrap();

    // 2. Lectura Lazy con Acceso Aleatorio O(1)
    let reader = ParcodeReader::open(file.path()).unwrap();
    let lazy_db = reader.read_lazy::<UserDatabase>().unwrap();

    // A. Búsqueda Exitosa (Random Access)
    let target_key = "user_4242".to_string();
    let profile = lazy_db
        .users
        .get(&target_key)
        .unwrap()
        .expect("User should exist");

    assert_eq!(profile.level, 4242 % 100);
    assert_eq!(profile.score, 42420);

    // B. Búsqueda Fallida (No existe)
    let missing = lazy_db.users.get(&"admin_root".to_string()).unwrap();
    assert!(missing.is_none());

    // C. Carga Completa (Fallback a Vec<(K,V)> -> HashMap)
    // El método .load() devuelve el HashMap completo reconstruido
    let loaded_map = lazy_db.users.load().unwrap();
    assert_eq!(loaded_map.len(), 5000);
    assert_eq!(loaded_map["user_100"], users["user_100"]);
}
